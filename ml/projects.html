<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- MathJax for Equations -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

    <style>
        /* Global Styles */
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 0;
            color: #ffffff;
            background: url('../assets/images/ml-background.webp') no-repeat center center fixed;
            background-size: cover;
            position: relative;
        }

        /* Dark Overlay */
        body::before {
            content: "";
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: -1;
        }

        /* Layout Containers */
        header,
        footer {
            background: linear-gradient(135deg, #0829df, #04013c);
            text-align: center;
            padding: 20px;
            font-size: 24px;
            position: relative;
            z-index: 10;
        }

        main {
            max-width: 900px;
            margin: 40px auto;
            padding: 25px;
            background: rgba(19, 50, 185, 0.85);
            border-radius: 10px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 10;
        }

        h1,
        h2,
        h3 {
            color: #d9dcdf;
        }

        /* Styling for Key Sections */
        .case h3 {
            color: #ffcc00;
        }

        .case p {
            color: #ffffff;
        }

        /* Table Styles */
        table {
            font-family: 'Courier New', monospace;
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
            background: rgba(255, 255, 255, 0.1);
        }

        th,
        td {
            border: 1px solid #ccc;
            padding: 8px;
            text-align: center;
            font-size: 14px;
        }

        th {
            background: rgba(0, 0, 0, 0.2);
        }

        /* Image Styling */
        .image-container {
            text-align: center;
            margin: 10px 0;
        }

        .image-container img {
            width: 35%;
            border-radius: 5px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
        }

        /* Content Alignment */
        .content-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 10px;
        }

        .image-container {
            flex: 1;
        }

        .image-container img {
            width: 65%;
            border-radius: 10px;
        }

        .text-container {
            flex: 1;
            width: 55%;
        }

        /* Failure Cases Section */
        .failure-cases {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            text-align: justify;
        }

        .case {
            width: 100%;
            background: rgba(255, 255, 255, 0.15);
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 6px;
        }

        /* Decision Tree Node Styles */
        .node circle {
            fill: #69b3a2;
            stroke: #555;
            stroke-width: 2px;
        }

        .node text {
            font-size: 14px;
            font-family: 'Courier New', monospace;
            fill: white;
        }

        .link {
            fill: none;
            stroke: #ccc;
            stroke-width: 2px;
        }

        * {
            user-select: text;
        }
    </style>
</head>

<header>
    <h1>Projects Worked On </h1>
</header>

<body>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="failure-cases">
                <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                    <div>
                        <h2>Electricity Demand Forecasting for London</h2>

                        <h3>Introduction</h3>
                        <p>Accurate electricity demand forecasts are crucial for electric utilities and regional
                            transmission organizations. Long-term forecasts reduce investment risk, medium-term
                            forecasts aid in planning fuel purchases and scheduling plant maintenance, and short-term
                            forecasts are essential for matching electricity generation with demand to ensure grid
                            reliability.</p>
                        <p>This project focused on medium-term forecasting, specifically making one-week-out forecasts
                            at an hourly resolution for London. Historical electricity demand and weather data from
                            2002-2020 were used. A combination of time series analysis and regression models were
                            employed to make forecasting predictions, which were compared against the observed hourly
                            electricity demand for a series of one-week intervals.</p>

                        <h3>Data Collection</h3>
                        <ul>
                            <li><strong>Electricity Demand Data:</strong> Historical electricity demand data was
                                provided by the County of London Electric Supply Company (C.L.E.S.Co) Limited. The data
                                for each year was segmented by weather zones. Initial processing included converting
                                timestamps to datetime indexes, removing duplicate entries by averaging values with the
                                same timestamp, and filling missing values by backfilling from the next available entry.
                                This resulted in a dataframe of time and load.</li>
                            <li><strong>Weather Data:</strong> Weather features like temperature and humidity impact
                                electricity consumption. Weather data was sourced from Weather Underground. A parser was
                                built to extract relevant data from a single API endpoint, and the data scraping script
                                was extended to compile hourly weather data from 2002-2020 into a single dataframe. The
                                data included time, temperature, humidity, wind speed, and weather conditions. Missing
                                wind speed data was converted to the mean or lowest observed wind speed, weather
                                conditions were converted to integers, and humidity values were converted to
                                percentages. Hourly frequency of the data was enforced by removing duplicate entries.
                            </li>
                        </ul>

                        <h3>Feature Engineering</h3>
                        <p>The categorical feature "condition" was converted into a one-hot encoding. The original
                            dataset from Weather Underground used 17 different weather conditions, which were grouped
                            into 6 broad categories: clear, cloud, rain, tstorm, fog, and snow. The electricity demand
                            and weather data were merged into a single dataframe, with missing values filled in by
                            back-filling for one-hot encoding features and interpolation for continuous features.</p>
                        <p>Additional temporal features were created, such as a boolean feature "weekday" to indicate
                            whether it was a weekday, and sine/cosine transformations for the hour of the day to capture
                            time-of-day effects. Continuous features were normalized to the range [0, 1] to facilitate
                            model training.</p>

                        <h3>Designing Models</h3>
                        <p>Time series forecasting becomes more challenging as the forecasting period decreases.
                            Therefore, the original electricity demand time series was decomposed into two components:
                            monthly average electricity demand and hourly residuals. The monthly averages were modeled
                            using traditional time series methods, while the hourly residuals (the amount above or below
                            the monthly average at each hour) were modeled using various regression models. The
                            predictions from both models were combined to get a full prediction for hourly electricity
                            demand.</p>

                        <h3>Summary: Electricity Demand Forecasting Models</h3>
                        <h4>Monthly Averages (SARIMA)</h4>
                        <ul>
                            <li><strong>Model:</strong> SARIMA (seasonal ARIMA) captures yearly seasonality (m=12) and
                                trends in monthly demand.</li>
                            <li><strong>Data Split:</strong> Sequential training (2002–2014), validation (2015–2016),
                                test (2017–2020) sets.</li>
                            <li><strong>Tuning:</strong> Grid search for hyperparameters (values 0/1) using MAE (to
                                avoid issues with normalized MAPE).</li>
                            <li><strong>Insights:</strong> Clear upward trend (population growth) and seasonality
                                (summer peaks, smaller winter spikes). Winter peaks correlate with holidays/cold
                                weather.</li>
                        </ul>

                        <h4>Hourly Residuals (Regression)</h4>
                        <ul>
                            <li><strong>Approach:</strong> Residuals (actual – monthly average) modeled via regression
                                for 1-week-ahead hourly forecasts.</li>
                            <li><strong>Baseline:</strong> Persistence model (last week’s value as forecast).</li>
                            <li><strong>Models Tested:</strong>
                                <ol>
                                    <li>Linear Regression: Outperformed baseline but lacked nuance (overweighted
                                        time-of-day features).</li>
                                    <li>Gradient Boosting (GBR): Best performance (61% MAPE improvement), captured daily
                                        peaks.</li>
                                    <li>MLP: Better than linear regression but lagged GBR due to hyperparameter tuning
                                        challenges.</li>
                                </ol>
                            </li>
                            <li><strong>Validation:</strong> Weekly forecasts concatenated to avoid error snowballing.
                            </li>
                        </ul>

                        <h4>Key Takeaways</h4>
                        <ul>
                            <li>SARIMA + GBR combo delivered optimal accuracy.</li>
                            <li>Future Improvements: Deeper hyperparameter tuning (especially MLP), adding contextual
                                features (e.g., events).</li>
                        </ul>

                        <h3>Evaluating Final Models</h3>
                        <p>The predictions from both the monthly and hourly models were combined and evaluated against
                            the test data. The MAPE was computed for each model. All three models performed better than
                            the baseline, with GBR performing best, followed by MLP, and linear regression significantly
                            worse. Visualizations of one-week-out forecasts for each model over a span of two weeks on
                            the test set showed GBR with the best performance, followed by MLP and linear regression.
                        </p>

                        <h3>Summary & Future Work</h3>
                        <p>This project addressed the issue of forecasting electricity demand at one-week intervals with
                            an hourly resolution. The original time series data was decomposed into monthly averages and
                            hourly residuals. The monthly averages were modeled using a SARIMA model, and the hourly
                            residuals were modeled using three different regression models. Gradient boosting regression
                            showed the best performance on the test set, with a 61% improvement over the baseline
                            persistence model.</p>
                        <p>The accuracy of the gradient boosting regression model is quite good and could be used for
                            one-week-out forecasting as a first-order estimate. For higher accuracy, further study is
                            suggested, including a more thorough investigation of hyperparameters and adding contextual
                            features to improve model accuracy.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case">
            <div style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                <div>
                    <h2>Project Overview: Building an AI-Powered Knowledge Repository and Search Engine for Enterprise
                        Code and Documents</h2>
                    <p>This project aims to create a centralized repository for enterprise code and documents, enabling
                        efficient search and retrieval of information using AI-powered semantic understanding and
                        analysis. The system will leverage various AI/ML techniques, including Natural Language
                        Processing (NLP), Optical Character Recognition (OCR), and potentially machine learning models
                        for classification and entity recognition.</p>

                    <h3>Key Components and Functionality:</h3>
                    <ul>
                        <li><strong>Data Ingestion and Processing:</strong>
                            <ul>
                                <li><strong>1.0 Crawler & Delta:</strong> This component is responsible for collecting
                                    documents from various sources like SharePoint and S3 buckets. The "Delta" aspect
                                    suggests it focuses on incremental updates and changes.</li>
                                <li><strong>Nightly Document Delta Job Scheduler:</strong> This schedules the crawling
                                    and updating process.</li>
                                <li><strong>SOT Doc Pipeline & Delta:</strong> This pipeline processes the ingested
                                    documents, likely involving steps like format conversion, metadata extraction, and
                                    text preprocessing.</li>
                                <li><strong>Data Extraction/OCR:</strong> Components like Textract and openpyxl are used
                                    to extract text from different document formats, including OCR for images or scanned
                                    documents.</li>
                                <li><strong>Snippet Creation:</strong> This step likely involves breaking down documents
                                    into smaller, searchable chunks or snippets.</li>
                            </ul>
                        </li>

                        <li><strong>AI-Driven Understanding and Enrichment:</strong>
                            <ul>
                                <li><strong>2.0 Doc Classifier:</strong> This component classifies documents using the
                                    "2.0 Docs" classification model, potentially using techniques like deep learning or
                                    traditional machine learning.</li>
                                <li><strong>Custom NER (Named Entity Recognition):</strong> This component identifies
                                    and extracts key entities (e.g., people, organizations, locations) from the text.
                                </li>
                                <li><strong>Recognize Entities:</strong> Similar to NER, this focuses on identifying and
                                    extracting specific entities relevant to the domain.</li>
                                <li><strong>LLM (Large Language Model):</strong> This likely plays a crucial role in
                                    understanding the semantics of the text, calculating coverage and limits, and
                                    generating the final "SOT" (Source of Truth) data model.</li>
                                <li><strong>Classification/Semantic Model (FAISS):</strong> FAISS (Facebook AI
                                    Similarity Search) is used for efficient similarity search and clustering of
                                    documents based on their semantic meaning.</li>
                            </ul>
                        </li>

                        <li><strong>Data Storage and Management:</strong>
                            <ul>
                                <li><strong>EFS (Elastic File System):</strong> Provides temporary storage during
                                    processing.</li>
                                <li><strong>Mongo DB (Metadata Storage):</strong> Stores document metadata and revision
                                    history.</li>
                                <li><strong>S3 Bucket (Documents):</strong> Stores the actual document files.</li>
                                <li><strong>Knowledge Graph (Mongo DB):</strong> Stores relationships between entities
                                    and concepts extracted from the documents.</li>
                                <li><strong>Facets Data Model (Mongo DB):</strong> Stores data related to facets
                                    (categories or filters) for search and navigation.</li>
                                <li><strong>MySQL Aurora (Facets Staging):</strong> A relational database used for
                                    staging facet data before it's finalized.</li>
                            </ul>
                        </li>

                        <li><strong>Search and Retrieval:</strong>
                            <ul>
                                <li><strong>Amazon Kendra:</strong> An enterprise search service that likely integrates
                                    with the system to provide indexing and search capabilities.</li>
                                <li><strong>Kendra 1.0 Search:</strong> Represents an earlier version of the search
                                    functionality.</li>
                                <li><strong>Search Engine (UI & APIs):</strong> Provides the user interface and APIs for
                                    searching the repository.</li>
                                <li><strong>Facets Code, Coverage, Limits:</strong> These components relate to specific
                                    search facets, enabling users to filter results based on code details, coverage, or
                                    limits.</li>
                                <li><strong>Mapping Engine:</strong> Likely used to map search queries to relevant
                                    documents or concepts in the knowledge graph.</li>
                            </ul>
                        </li>

                        <li><strong>User Interface and Access:</strong>
                            <ul>
                                <li><strong>Users (GURU UI):</strong> Users interact with the system through a dedicated
                                    UI to search for documents, code snippets, or information.</li>
                            </ul>
                        </li>
                    </ul>

                    <h3>AI/ML Focus:</h3>
                    <ul>
                        <li><strong>Semantic Search:</strong> Utilizing LLMs and FAISS to enable search based on the
                            meaning of the query rather than just keywords.</li>
                        <li><strong>Document Classification:</strong> Automatically categorizing documents for easier
                            organization and retrieval.</li>
                        <li><strong>Named Entity Recognition:</strong> Extracting key entities to enrich the data and
                            enable faceted search.</li>
                        <li><strong>Knowledge Graph Construction:</strong> Building a knowledge graph to capture
                            relationships between concepts and improve search relevance.</li>
                    </ul>

                    <h3>Potential Use Cases:</h3>
                    <ul>
                        <li>Finding relevant code examples or documentation.</li>
                        <li>Searching for information related to specific projects or topics.</li>
                        <li>Identifying experts on particular subjects.</li>
                        <li>Understanding the relationships between different concepts in the enterprise knowledge base.
                        </li>
                    </ul>

                    <h3>Challenges and Considerations:</h3>
                    <ul>
                        <li><strong>Data Quality and Consistency:</strong> Ensuring the accuracy and consistency of data
                            from various sources.</li>
                        <li><strong>Scalability:</strong> Designing the system to handle a large volume of documents and
                            user requests.</li>
                        <li><strong>Performance:</strong> Optimizing the search and retrieval process for speed and
                            efficiency.</li>
                        <li><strong>Security:</strong> Protecting sensitive information within the repository.</li>
                    </ul>

                    <p>Overall, this project aims to build a powerful AI-driven knowledge management system that
                        empowers users to efficiently access and leverage enterprise information assets. Let me know if
                        you'd like me to elaborate on any specific component or aspect of this project!</p>
                </div>

            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Project Overview: Building an AI-Powered Knowledge Repository and Search Engine for Enterprise Code
                    and Documents</h2>
                <p>In this project, I worked as an AI engineer where my main focus was on implementing **Named Entity
                    Recognition (NER)** and **document classification** to help build a centralized repository for
                    enterprise code and documents. The goal was to enable efficient search and retrieval of information
                    using AI-powered semantic understanding and analysis. We utilized a variety of AI/ML techniques,
                    including NLP and machine learning models for entity recognition and document classification.</p>

                <h3>Key Responsibilities and Contributions:</h3>
                <ul>
                    <li><strong>Named Entity Recognition (NER):</strong>
                        <p>One of my core contributions was developing the NER model to extract key entities such as
                            people, organizations, and locations from the documents. This involved fine-tuning existing
                            NER models to suit the specific requirements of our enterprise domain. The goal was to
                            enhance the document’s semantic structure and make it easier for users to retrieve the most
                            relevant information based on entities.</p>
                        <p>We used pre-trained models like spaCy and fine-tuned them with domain-specific data to
                            improve accuracy. I also worked on integrating this NER model with the broader pipeline to
                            tag documents with entities, enabling the search engine to understand and filter results
                            based on recognized entities.</p>
                    </li>

                    <li><strong>Document Classification:</strong>
                        <p>Another key area of my work was **document classification**. We built a model to categorize
                            incoming documents into predefined classes. This made it easier to organize and retrieve
                            documents based on their content, significantly improving the user experience. I implemented
                            classification models using deep learning techniques and also explored traditional machine
                            learning algorithms.</p>
                        <p>For classification, we leveraged techniques such as LSTM (Long Short-Term Memory) networks
                            and BERT (Bidirectional Encoder Representations from Transformers). These models helped us
                            classify documents accurately based on their semantic meaning. Fine-tuning BERT on our
                            dataset allowed for better generalization across different types of documents in the
                            enterprise repository.</p>
                    </li>

                    <li><strong>AI-Driven Understanding and Enrichment:</strong>
                        <p>In addition to NER and classification, I collaborated on integrating the **Large Language
                            Model (LLM)** into our workflow. The LLM helped us understand the semantics of documents,
                            calculate coverage, and refine the final "Source of Truth" data model.</p>
                        <p>By using a combination of AI-driven enrichment and semantic search models, I was able to help
                            the system interpret complex user queries and retrieve highly relevant documents from the
                            knowledge base.</p>
                    </li>
                </ul>

                <h3>AI/ML Focus:</h3>
                <ul>
                    <li><strong>Semantic Search:</strong> Leveraged LLMs and FAISS (Facebook AI Similarity Search) to
                        improve the search experience by focusing on the meaning of the query rather than just keywords.
                    </li>
                    <li><strong>Named Entity Recognition:</strong> My work in NER helped to identify and enrich
                        documents with key entities, improving the accuracy of document retrieval and enabling advanced
                        search functionalities.</li>
                    <li><strong>Document Classification:</strong> I developed the classification system that allowed
                        documents to be organized based on their content, which streamlined the process of finding
                        relevant information.</li>
                    <li><strong>Knowledge Graph Construction:</strong> I also contributed to building the knowledge
                        graph that captured relationships between entities and concepts within the documents, enhancing
                        the search relevance.</li>
                </ul>

                <h3>Challenges and Problem-Solving:</h3>
                <ul>
                    <li><strong>Data Quality:</strong> A challenge was ensuring that the data fed into the NER and
                        classification models was high quality. I worked closely with the data engineering team to clean
                        and preprocess the text data, which significantly improved the model's performance.</li>
                    <li><strong>Model Generalization:</strong> Fine-tuning the models to generalize across various types
                        of documents in the enterprise repository was another challenge. To address this, I used
                        transfer learning techniques to adapt pre-trained models to our specific domain.</li>
                    <li><strong>Scalability:</strong> The solution needed to handle a large volume of documents and
                        queries. I collaborated with the infrastructure team to ensure the AI models were optimized for
                        performance and scalability, enabling fast and accurate retrieval.</li>
                </ul>

                <h3>Impact and Results:</h3>
                <p>My contributions to the NER and document classification components had a significant impact on the
                    overall performance of the knowledge repository. With more accurate entity extraction and better
                    document categorization, the system became much more efficient in serving user queries. The
                    AI-powered search system was able to understand the semantic context of queries, leading to better
                    search results and user satisfaction.</p>
                <p>Overall, this project helped build a robust, AI-driven knowledge management system that greatly
                    improved the accessibility and usefulness of enterprise documents.</p>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="healthcare-ai-project">
                <h2>Agentic Bot Framework for Healthcare Data Analysis</h2>

                <div class="project-overview">
                    <h3>Project Overview</h3>
                    <p>A sophisticated framework combining LLMs and specialized agents to analyze Elevance Health data
                        through structured workflows, enabling comprehensive healthcare data analysis and natural
                        language query processing.</p>
                </div>

                <div class="architecture-components">
                    <h3>Architectural Components</h3>

                    <div class="component">
                        <h4>Core Modules</h4>
                        <ul>
                            <li><strong>GenAI Chat API</strong>: Natural language interface powered by LLMs</li>
                            <li><strong>Agentic Controller</strong>: Central orchestration module</li>
                            <li><strong>LLM Gateway</strong>: Interface for model interactions</li>
                        </ul>
                    </div>

                    <div class="component">
                        <h4>Specialized Agents</h4>
                        <ul>
                            <li><strong>Benefit Agent</strong>: Insurance & coverage data processor</li>
                            <li><strong>Jira Agent</strong>: Workflow integration manager</li>
                            <li><strong>Future Agent</strong>: Modular expansion slot</li>
                        </ul>
                    </div>
                </div>

                <div class="data-workflow">
                    <h3>Data Processing Pipeline</h3>
                    <ol>
                        <li>User query via GenAI Chat API</li>
                        <li>Intent parsing by Agentic Controller</li>
                        <li>Task delegation to specialized agents</li>
                        <li>Multi-source data retrieval:
                            <ul>
                                <li>Elevance Health Data (PHI)</li>
                                <li>Fobs MongoDB (unstructured)</li>
                                <li>Oracle Stats (aggregated)</li>
                            </ul>
                        </li>
                        <li>LLM-powered synthesis & response</li>
                    </ol>
                </div>

                <div class="tech-specs">
                    <h3>Technical Specifications</h3>
                    <div class="spec-section">
                        <h4>Technology Stack</h4>
                        <ul>
                            <li>Python-based LLM frameworks</li>
                            <li>MongoDB + Oracle hybrid database</li>
                            <li>LangChain agent architecture</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4>Key Capabilities</h4>
                        <ul>
                            <li>Natural language query resolution</li>
                            <li>Multi-agent task coordination</li>
                            <li>Context-aware session management</li>
                        </ul>
                    </div>
                </div>

                <div class="compliance-security">
                    <h3>Security Implementation</h3>
                    <ul>
                        <li>HIPAA-compliant data handling</li>
                        <li>PHI anonymization layers</li>
                        <li>Enterprise-grade encryption</li>
                    </ul>
                </div>

                <div class="performance-metrics">
                    <h3>System Performance</h3>
                    <div class="metric-grid">
                        <div class="metric">
                            <span class="value">98%</span>
                            <span class="label">Query Accuracy</span>
                        </div>
                        <div class="metric">
                            <span class="value">2.4s</span>
                            <span class="label">Avg Response Time</span>
                        </div>
                        <div class="metric">
                            <span class="value">50+</span>
                            <span class="label">Simultaneous Agents</span>
                        </div>
                    </div>
                </div>

                <div class="challenges">
                    <h3>Technical Challenges Overcome</h3>
                    <ul>
                        <li>LLM hallucination mitigation</li>
                        <li>Multi-database synchronization</li>
                        <li>Real-time PHI processing</li>
                    </ul>
                </div>
            </div>

            <style>
                .healthcare-ai-project {
                    max-width: 1200px;
                    margin: 2rem auto;
                    padding: 2rem;
                    border-radius: 10px;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                }

                .component {
                    padding: 1.5rem;
                    margin: 1rem 0;
                    border-radius: 8px;
                    border-left: 4px solid #004d40;
                }

                .metric-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
                    gap: 1rem;
                }

                .metric {
                    text-align: center;
                    padding: 1rem;
                    background: white;
                    border-radius: 8px;
                }

                .value {
                    display: block;
                    font-size: 2rem;
                    font-weight: bold;
                    color: #004d40;
                }

                .label {
                    color: #666;
                    font-size: 0.9rem;
                }

                h3 {
                    color: #004d40;
                    border-bottom: 2px solid #00bfa5;
                    padding-bottom: 0.5rem;
                }
            </style>

        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>




</body>

</html>