<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- MathJax for Equations -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

    <style>
        /* Global Styles */
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 0;
            color: #ffffff;
            background: url('../assets/images/ml-Background.jpg') no-repeat center center fixed;
            background-size: cover;
            position: relative;
        }

        /* Dark Overlay */
        body::before {
            content: "";
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: -1;
        }

        /* Layout Containers */
        header,
        footer {
            background: linear-gradient(135deg, #0829df, #04013c);
            text-align: center;
            padding: 20px;
            font-size: 24px;
            position: relative;
            z-index: 10;
        }

        main {
            max-width: 900px;
            margin: 40px auto;
            padding: 25px;
            background: rgba(19, 50, 185, 0.85);
            border-radius: 10px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 10;
        }

        h1,
        h2,
        h3 {
            color: #d9dcdf;
        }

        /* Styling for Key Sections */
        .case h3 {
            color: #ffcc00;
        }

        .case p {
            color: #ffffff;
        }

        /* Table Styles */
        table {
            font-family: 'Courier New', monospace;
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
            background: rgba(255, 255, 255, 0.1);
        }

        th,
        td {
            border: 1px solid #ccc;
            padding: 8px;
            text-align: center;
            font-size: 14px;
        }

        th {
            background: rgba(0, 0, 0, 0.2);
        }

        /* Image Styling */
        .image-container {
            text-align: center;
            margin: 10px 0;
        }

        .image-container img {
            width: 35%;
            border-radius: 5px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
        }

        /* Content Alignment */
        .content-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 10px;
        }

        .image-container {
            flex: 1;
        }

        .image-container img {
            width: 65%;
            border-radius: 10px;
        }

        .text-container {
            flex: 1;
            width: 55%;
        }

        /* Failure Cases Section */
        .failure-cases {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            text-align: justify;
        }

        .case {
            width: 100%;
            background: rgba(255, 255, 255, 0.15);
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 6px;
        }

        /* Decision Tree Node Styles */
        .node circle {
            fill: #69b3a2;
            stroke: #555;
            stroke-width: 2px;
        }

        .node text {
            font-size: 14px;
            font-family: 'Courier New', monospace;
            fill: white;
        }

        .link {
            fill: none;
            stroke: #ccc;
            stroke-width: 2px;
        }

        * {
            user-select: text;
        }
    </style>
</head>

<header>
    <h1>Projects Worked On </h1>
</header>

<body>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        
                    <div>
                        <h2>Electricity Demand Forecasting for London</h2>

                        <h3>Introduction</h3>
                        <p>Accurate electricity demand forecasts are crucial for electric utilities and regional
                            transmission organizations. Long-term forecasts reduce investment risk, medium-term
                            forecasts aid in planning fuel purchases and scheduling plant maintenance, and short-term
                            forecasts are essential for matching electricity generation with demand to ensure grid
                            reliability.</p>
                        <p>This project focused on medium-term forecasting, specifically making one-week-out forecasts
                            at an hourly resolution for London. Historical electricity demand and weather data from
                            2002-2020 were used. A combination of time series analysis and regression models were
                            employed to make forecasting predictions, which were compared against the observed hourly
                            electricity demand for a series of one-week intervals.</p>

                        <h3>Data Collection</h3>
                        <ul>
                            <li><strong>Electricity Demand Data:</strong> Historical electricity demand data was
                                provided by the County of London Electric Supply Company (C.L.E.S.Co) Limited. The data
                                for each year was segmented by weather zones. Initial processing included converting
                                timestamps to datetime indexes, removing duplicate entries by averaging values with the
                                same timestamp, and filling missing values by backfilling from the next available entry.
                                This resulted in a dataframe of time and load.</li>
                            <li><strong>Weather Data:</strong> Weather features like temperature and humidity impact
                                electricity consumption. Weather data was sourced from Weather Underground. A parser was
                                built to extract relevant data from a single API endpoint, and the data scraping script
                                was extended to compile hourly weather data from 2002-2020 into a single dataframe. The
                                data included time, temperature, humidity, wind speed, and weather conditions. Missing
                                wind speed data was converted to the mean or lowest observed wind speed, weather
                                conditions were converted to integers, and humidity values were converted to
                                percentages. Hourly frequency of the data was enforced by removing duplicate entries.
                            </li>
                        </ul>

                        <h3>Feature Engineering</h3>
                        <p>The categorical feature "condition" was converted into a one-hot encoding. The original
                            dataset from Weather Underground used 17 different weather conditions, which were grouped
                            into 6 broad categories: clear, cloud, rain, tstorm, fog, and snow. The electricity demand
                            and weather data were merged into a single dataframe, with missing values filled in by
                            back-filling for one-hot encoding features and interpolation for continuous features.</p>
                        <p>Additional temporal features were created, such as a boolean feature "weekday" to indicate
                            whether it was a weekday, and sine/cosine transformations for the hour of the day to capture
                            time-of-day effects. Continuous features were normalized to the range [0, 1] to facilitate
                            model training.</p>

                        <h3>Designing Models</h3>
                        <p>Time series forecasting becomes more challenging as the forecasting period decreases.
                            Therefore, the original electricity demand time series was decomposed into two components:
                            monthly average electricity demand and hourly residuals. The monthly averages were modeled
                            using traditional time series methods, while the hourly residuals (the amount above or below
                            the monthly average at each hour) were modeled using various regression models. The
                            predictions from both models were combined to get a full prediction for hourly electricity
                            demand.</p>

                        <h3>Summary: Electricity Demand Forecasting Models</h3>
                        <h4>Monthly Averages (SARIMA)</h4>
                        <ul>
                            <li><strong>Model:</strong> SARIMA (seasonal ARIMA) captures yearly seasonality (m=12) and
                                trends in monthly demand.</li>
                            <li><strong>Data Split:</strong> Sequential training (2002–2014), validation (2015–2016),
                                test (2017–2020) sets.</li>
                            <li><strong>Tuning:</strong> Grid search for hyperparameters (values 0/1) using MAE (to
                                avoid issues with normalized MAPE).</li>
                            <li><strong>Insights:</strong> Clear upward trend (population growth) and seasonality
                                (summer peaks, smaller winter spikes). Winter peaks correlate with holidays/cold
                                weather.</li>
                        </ul>

                        <h4>Hourly Residuals (Regression)</h4>
                        <ul>
                            <li><strong>Approach:</strong> Residuals (actual – monthly average) modeled via regression
                                for 1-week-ahead hourly forecasts.</li>
                            <li><strong>Baseline:</strong> Persistence model (last week’s value as forecast).</li>
                            <li><strong>Models Tested:</strong>
                                <ol>
                                    <li>Linear Regression: Outperformed baseline but lacked nuance (overweighted
                                        time-of-day features).</li>
                                    <li>Gradient Boosting (GBR): Best performance (61% MAPE improvement), captured daily
                                        peaks.</li>
                                    <li>MLP: Better than linear regression but lagged GBR due to hyperparameter tuning
                                        challenges.</li>
                                </ol>
                            </li>
                            <li><strong>Validation:</strong> Weekly forecasts concatenated to avoid error snowballing.
                            </li>
                        </ul>

                        <h4>Key Takeaways</h4>
                        <ul>
                            <li>SARIMA + GBR combo delivered optimal accuracy.</li>
                            <li>Future Improvements: Deeper hyperparameter tuning (especially MLP), adding contextual
                                features (e.g., events).</li>
                        </ul>

                        <h3>Evaluating Final Models</h3>
                        <p>The predictions from both the monthly and hourly models were combined and evaluated against
                            the test data. The MAPE was computed for each model. All three models performed better than
                            the baseline, with GBR performing best, followed by MLP, and linear regression significantly
                            worse. Visualizations of one-week-out forecasts for each model over a span of two weeks on
                            the test set showed GBR with the best performance, followed by MLP and linear regression.
                        </p>

                        <h3>Summary & Future Work</h3>
                        <p>This project addressed the issue of forecasting electricity demand at one-week intervals with
                            an hourly resolution. The original time series data was decomposed into monthly averages and
                            hourly residuals. The monthly averages were modeled using a SARIMA model, and the hourly
                            residuals were modeled using three different regression models. Gradient boosting regression
                            showed the best performance on the test set, with a 61% improvement over the baseline
                            persistence model.</p>
                        <p>The accuracy of the gradient boosting regression model is quite good and could be used for
                            one-week-out forecasting as a first-order estimate. For higher accuracy, further study is
                            suggested, including a more thorough investigation of hyperparameters and adding contextual
                            features to improve model accuracy.</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case">
            <div style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                <div>
                    <h2>Project Overview: Building an AI-Powered Knowledge Repository and Search Engine for Enterprise
                        Code and Documents</h2>
                    <p>This project aims to create a centralized repository for enterprise code and documents, enabling
                        efficient search and retrieval of information using AI-powered semantic understanding and
                        analysis. The system will leverage various AI/ML techniques, including Natural Language
                        Processing (NLP), Optical Character Recognition (OCR), and potentially machine learning models
                        for classification and entity recognition.</p>

                    <h3>Key Components and Functionality:</h3>
                    <ul>
                        <li><strong>Data Ingestion and Processing:</strong>
                            <ul>
                                <li><strong>1.0 Crawler & Delta:</strong> This component is responsible for collecting
                                    documents from various sources like SharePoint and S3 buckets. The "Delta" aspect
                                    suggests it focuses on incremental updates and changes.</li>
                                <li><strong>Nightly Document Delta Job Scheduler:</strong> This schedules the crawling
                                    and updating process.</li>
                                <li><strong>SOT Doc Pipeline & Delta:</strong> This pipeline processes the ingested
                                    documents, likely involving steps like format conversion, metadata extraction, and
                                    text preprocessing.</li>
                                <li><strong>Data Extraction/OCR:</strong> Components like Textract and openpyxl are used
                                    to extract text from different document formats, including OCR for images or scanned
                                    documents.</li>
                                <li><strong>Snippet Creation:</strong> This step likely involves breaking down documents
                                    into smaller, searchable chunks or snippets.</li>
                            </ul>
                        </li>

                        <li><strong>AI-Driven Understanding and Enrichment:</strong>
                            <ul>
                                <li><strong>2.0 Doc Classifier:</strong> This component classifies documents using the
                                    "2.0 Docs" classification model, potentially using techniques like deep learning or
                                    traditional machine learning.</li>
                                <li><strong>Custom NER (Named Entity Recognition):</strong> This component identifies
                                    and extracts key entities (e.g., people, organizations, locations) from the text.
                                </li>
                                <li><strong>Recognize Entities:</strong> Similar to NER, this focuses on identifying and
                                    extracting specific entities relevant to the domain.</li>
                                <li><strong>LLM (Large Language Model):</strong> This likely plays a crucial role in
                                    understanding the semantics of the text, calculating coverage and limits, and
                                    generating the final "SOT" (Source of Truth) data model.</li>
                                <li><strong>Classification/Semantic Model (FAISS):</strong> FAISS (Facebook AI
                                    Similarity Search) is used for efficient similarity search and clustering of
                                    documents based on their semantic meaning.</li>
                            </ul>
                        </li>

                        <li><strong>Data Storage and Management:</strong>
                            <ul>
                                <li><strong>EFS (Elastic File System):</strong> Provides temporary storage during
                                    processing.</li>
                                <li><strong>Mongo DB (Metadata Storage):</strong> Stores document metadata and revision
                                    history.</li>
                                <li><strong>S3 Bucket (Documents):</strong> Stores the actual document files.</li>
                                <li><strong>Knowledge Graph (Mongo DB):</strong> Stores relationships between entities
                                    and concepts extracted from the documents.</li>
                                <li><strong>Facets Data Model (Mongo DB):</strong> Stores data related to facets
                                    (categories or filters) for search and navigation.</li>
                                <li><strong>MySQL Aurora (Facets Staging):</strong> A relational database used for
                                    staging facet data before it's finalized.</li>
                            </ul>
                        </li>

                        <li><strong>Search and Retrieval:</strong>
                            <ul>
                                <li><strong>Amazon Kendra:</strong> An enterprise search service that likely integrates
                                    with the system to provide indexing and search capabilities.</li>
                                <li><strong>Kendra 1.0 Search:</strong> Represents an earlier version of the search
                                    functionality.</li>
                                <li><strong>Search Engine (UI & APIs):</strong> Provides the user interface and APIs for
                                    searching the repository.</li>
                                <li><strong>Facets Code, Coverage, Limits:</strong> These components relate to specific
                                    search facets, enabling users to filter results based on code details, coverage, or
                                    limits.</li>
                                <li><strong>Mapping Engine:</strong> Likely used to map search queries to relevant
                                    documents or concepts in the knowledge graph.</li>
                            </ul>
                        </li>

                        <li><strong>User Interface and Access:</strong>
                            <ul>
                                <li><strong>Users (GURU UI):</strong> Users interact with the system through a dedicated
                                    UI to search for documents, code snippets, or information.</li>
                            </ul>
                        </li>
                    </ul>

                    <h3>AI/ML Focus:</h3>
                    <ul>
                        <li><strong>Semantic Search:</strong> Utilizing LLMs and FAISS to enable search based on the
                            meaning of the query rather than just keywords.</li>
                        <li><strong>Document Classification:</strong> Automatically categorizing documents for easier
                            organization and retrieval.</li>
                        <li><strong>Named Entity Recognition:</strong> Extracting key entities to enrich the data and
                            enable faceted search.</li>
                        <li><strong>Knowledge Graph Construction:</strong> Building a knowledge graph to capture
                            relationships between concepts and improve search relevance.</li>
                    </ul>

                    <h3>Potential Use Cases:</h3>
                    <ul>
                        <li>Finding relevant code examples or documentation.</li>
                        <li>Searching for information related to specific projects or topics.</li>
                        <li>Identifying experts on particular subjects.</li>
                        <li>Understanding the relationships between different concepts in the enterprise knowledge base.
                        </li>
                    </ul>

                    <h3>Challenges and Considerations:</h3>
                    <ul>
                        <li><strong>Data Quality and Consistency:</strong> Ensuring the accuracy and consistency of data
                            from various sources.</li>
                        <li><strong>Scalability:</strong> Designing the system to handle a large volume of documents and
                            user requests.</li>
                        <li><strong>Performance:</strong> Optimizing the search and retrieval process for speed and
                            efficiency.</li>
                        <li><strong>Security:</strong> Protecting sensitive information within the repository.</li>
                    </ul>

                    <p>Overall, this project aims to build a powerful AI-driven knowledge management system that
                        empowers users to efficiently access and leverage enterprise information assets. Let me know if
                        you'd like me to elaborate on any specific component or aspect of this project!</p>
                </div>

            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Project Overview: Building an AI-Powered Knowledge Repository and Search Engine for Enterprise Code
                    and Documents</h2>
                <p>In this project, I worked as an AI engineer where my main focus was on implementing **Named Entity
                    Recognition (NER)** and **document classification** to help build a centralized repository for
                    enterprise code and documents. The goal was to enable efficient search and retrieval of information
                    using AI-powered semantic understanding and analysis. We utilized a variety of AI/ML techniques,
                    including NLP and machine learning models for entity recognition and document classification.</p>

                <h3>Key Responsibilities and Contributions:</h3>
                <ul>
                    <li><strong>Named Entity Recognition (NER):</strong>
                        <p>One of my core contributions was developing the NER model to extract key entities such as
                            people, organizations, and locations from the documents. This involved fine-tuning existing
                            NER models to suit the specific requirements of our enterprise domain. The goal was to
                            enhance the document’s semantic structure and make it easier for users to retrieve the most
                            relevant information based on entities.</p>
                        <p>We used pre-trained models like spaCy and fine-tuned them with domain-specific data to
                            improve accuracy. I also worked on integrating this NER model with the broader pipeline to
                            tag documents with entities, enabling the search engine to understand and filter results
                            based on recognized entities.</p>
                    </li>

                    <li><strong>Document Classification:</strong>
                        <p>Another key area of my work was **document classification**. We built a model to categorize
                            incoming documents into predefined classes. This made it easier to organize and retrieve
                            documents based on their content, significantly improving the user experience. I implemented
                            classification models using deep learning techniques and also explored traditional machine
                            learning algorithms.</p>
                        <p>For classification, we leveraged techniques such as LSTM (Long Short-Term Memory) networks
                            and BERT (Bidirectional Encoder Representations from Transformers). These models helped us
                            classify documents accurately based on their semantic meaning. Fine-tuning BERT on our
                            dataset allowed for better generalization across different types of documents in the
                            enterprise repository.</p>
                    </li>

                    <li><strong>AI-Driven Understanding and Enrichment:</strong>
                        <p>In addition to NER and classification, I collaborated on integrating the **Large Language
                            Model (LLM)** into our workflow. The LLM helped us understand the semantics of documents,
                            calculate coverage, and refine the final "Source of Truth" data model.</p>
                        <p>By using a combination of AI-driven enrichment and semantic search models, I was able to help
                            the system interpret complex user queries and retrieve highly relevant documents from the
                            knowledge base.</p>
                    </li>
                </ul>

                <h3>AI/ML Focus:</h3>
                <ul>
                    <li><strong>Semantic Search:</strong> Leveraged LLMs and FAISS (Facebook AI Similarity Search) to
                        improve the search experience by focusing on the meaning of the query rather than just keywords.
                    </li>
                    <li><strong>Named Entity Recognition:</strong> My work in NER helped to identify and enrich
                        documents with key entities, improving the accuracy of document retrieval and enabling advanced
                        search functionalities.</li>
                    <li><strong>Document Classification:</strong> I developed the classification system that allowed
                        documents to be organized based on their content, which streamlined the process of finding
                        relevant information.</li>
                    <li><strong>Knowledge Graph Construction:</strong> I also contributed to building the knowledge
                        graph that captured relationships between entities and concepts within the documents, enhancing
                        the search relevance.</li>
                </ul>

                <h3>Challenges and Problem-Solving:</h3>
                <ul>
                    <li><strong>Data Quality:</strong> A challenge was ensuring that the data fed into the NER and
                        classification models was high quality. I worked closely with the data engineering team to clean
                        and preprocess the text data, which significantly improved the model's performance.</li>
                    <li><strong>Model Generalization:</strong> Fine-tuning the models to generalize across various types
                        of documents in the enterprise repository was another challenge. To address this, I used
                        transfer learning techniques to adapt pre-trained models to our specific domain.</li>
                    <li><strong>Scalability:</strong> The solution needed to handle a large volume of documents and
                        queries. I collaborated with the infrastructure team to ensure the AI models were optimized for
                        performance and scalability, enabling fast and accurate retrieval.</li>
                </ul>

                <h3>Impact and Results:</h3>
                <p>My contributions to the NER and document classification components had a significant impact on the
                    overall performance of the knowledge repository. With more accurate entity extraction and better
                    document categorization, the system became much more efficient in serving user queries. The
                    AI-powered search system was able to understand the semantic context of queries, leading to better
                    search results and user satisfaction.</p>
                <p>Overall, this project helped build a robust, AI-driven knowledge management system that greatly
                    improved the accessibility and usefulness of enterprise documents.</p>
            </div>

        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="project-pitch">
                <h2>Agentic Bot Framework for Healthcare Account Plan Analysis</h2>
                <p>"One of my most impactful projects was developing an Agentic Bot Framework that streamlined
                    healthcare account plan data analysis using LlamaIndex. The goal was to create an intelligent system
                    comprised of specialized agents to analyze Elevance Health data through structured workflows,
                    enabling comprehensive healthcare data analysis and natural language query processing."</p>

                <h3>Problem Statement:</h3>
                <p>Healthcare account plans often contain vast amounts of fragmented data—ranging from benefit names to
                    cost shares across different network types. Navigating this data, validating its distribution, and
                    extracting meaningful insights manually is not only time-consuming but also prone to errors.</p>

                <p><strong>Our mission?</strong> Build an intelligent, agent-driven framework that simplifies plan
                    information access, enables real-time data validation, and even integrates operational tools like
                    Jira for task management.</p>

                <h3>Architecture Overview:</h3>

                <h4>Data Ingestion & Storage:</h4>
                <ul>
                    <li>Since the FOBS data was stored in MongoDB with each benefit name as a key, there was no need for
                        data chunking.</li>
                    <li>Using OpenAI embeddings, we processed the benefit name data directly, enabling semantic
                        understanding and allowing the system to retrieve data contextually rather than relying on
                        simple keyword matching.</li>
                    <li>The processed embeddings were stored in Postgres as a Vector DB, providing fast and accurate
                        similarity searches for user queries.</li>
                </ul>

                <h4>Validation Layer:</h4>
                <p>Before making the data available to end-users, it passed through a robust validation engine that
                    ensured data quality and accuracy:</p>
                <ul>
                    <li>Group ID Validation – Verifying correct group associations.</li>
                    <li>Benefit Name Validation – Ensuring benefit names match across datasets.</li>
                    <li>Effective Date Validation – Validating plan timelines and coverage periods.</li>
                </ul>

                <h4>Agentic Bot – User Interaction:</h4>
                <p>The Agentic Bot served as the primary interface for users, offering several powerful tools:</p>
                <ul>
                    <li><strong>Tool 1:</strong> Get Plan Info – Users could input any benefit name and instantly
                        retrieve detailed plan information.</li>
                    <li><strong>Tool 2:</strong> Get Statistics – This tool allowed users to generate statistical
                        summaries for specific groups, plans, or benefits, helping them make data-driven decisions.</li>
                    <li><strong>Tool 3:</strong> Engage Advisor via Jira – Using a Java API for Jira, users could create
                        tickets, track issue statuses, or escalate tasks directly from the bot.</li>
                </ul>

                <h4>Seamless Jira Integration:</h4>
                <p>The Java API for Jira was crucial in connecting our system with project management workflows. It
                    allowed the bot to not only log issues but also monitor ticket progress, ensuring smooth
                    communication between data teams and operational staff.</p>

                <h3>Workflows:</h3>
                <ul>
                    <p>User is assigned with a session ID to track interactions with the bot.</p>
                    <li><strong>User Query:</strong> Any information requested related to a benefit name.</li>
                    <li><strong>Bot Internal:</strong> Analyzes the query and extracts entities like benefit name, group
                        ID, plan name, and contract code.
                        <ul>
                            <li>If group ID is missing:
                                <ul>
                                    <li><strong>Bot Response:</strong> Requests the user to provide the group ID.</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    <li><strong>User Query:</strong> Provides group ID.</li>
                    <li><strong>Bot Internal:</strong> Validates the group ID, checks its association with the account,
                        and stores it in Redis for session management.
                        <ul>
                            <li>Once all mandatory metadata is available, the bot queries the database to retrieve
                                benefit details.</li>
                            <li>The top 5 results are sent to the LLM along with the user’s question.</li>
                        </ul>
                    </li>
                    <li><strong>Bot Response:</strong> The LLM generates an answer based on the top 5 results.</li>
                    <li><strong>User Query:</strong> If the user requests statistics for a group.</li>
                    <li><strong>Bot Internal:</strong> Executes stored procedures using the metadata, retrieves the
                        data, and sends it to the LLM to generate statistics in a specified format.</li>
                </ul>

                <h3>How It All Comes Together:</h3>
                <p>"Picture a healthcare analyst needing to verify cost-sharing details for a specific benefit. With the
                    Agentic Bot, they simply input the benefit name, and within seconds, the bot fetches validated plan
                    information, complete with timelines and cost breakdowns. If discrepancies arise, the analyst can
                    create a Jira ticket directly through the bot, streamlining issue tracking and resolution—all
                    without switching between multiple systems."</p>

                <h3>Key Technologies & Methodologies:</h3>
                <ul>
                    <li>LlamaIndex for structuring and querying complex healthcare data.</li>
                    <li>OpenAI Embeddings for semantic understanding and data processing.</li>
                    <li>Oracle, MongoDB & Postgres Vector DB for efficient data storage and retrieval.</li>
                    <li>Java API for seamless Jira integration.</li>
                    <li>Robust validation pipelines ensuring data integrity.</li>
                </ul>

                <h3>Impact & Achievements:</h3>
                <ul>
                    <li>60% reduction in data retrieval and validation time.</li>
                    <li>Improved data accuracy through multi-layered validation.</li>
                    <li>Enhanced user experience with intuitive tools, leading to faster decision-making.</li>
                    <li>Streamlined operations by integrating Jira, reducing communication gaps between analysts and
                        technical teams.</li>
                </ul>

                <h3>Final Thought:</h3>
                <p>"This project showcased my ability to blend AI, data engineering, and workflow automation to solve
                    real-world problems. It wasn’t just about building a bot; it was about creating a system that
                    empowered users to interact with complex healthcare data effortlessly while ensuring operational
                    efficiency."</p>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Project Overview: FOBS Data - Intent Prediction and Classification</h2>
                <p><strong>Objective:</strong><br>
                    The project aimed to process user-submitted "change text" from the FOBS system, predict its intent
                    using ELECTRA, and classify the text into two categories: <strong>"Benefit"</strong> or
                    <strong>"GBA"</strong>. This helped in automating workflows and reducing manual review.
                </p>
                <div>
                    <h2>Project Overview: FOBS Data - Intent Prediction, Classification, and Entity Extraction</h2>
                    <p><strong>Objective:</strong><br>
                        The project aimed to process user-submitted "change text" from the FOBS system, predict its
                        intent using ELECTRA, classify the text into two categories: <strong>"Benefit"</strong> or
                        <strong>"GBA"</strong>, and extract key entities using Rasa for deeper insights and downstream
                        processing.
                    </p>
                </div>

                <div>
                    <h3>Solution Approach</h3>
                    <ol>
                        <li><strong>Data Preprocessing:</strong>
                            <ul>
                                <li>Collected and cleaned the FOBS "change text" data.</li>
                                <li>Performed tokenization, removed special characters, and handled class imbalance.
                                </li>
                            </ul>
                        </li>
                        <li><strong>Intent Prediction using ELECTRA:</strong>
                            <ul>
                                <li>Fine-tuned the ELECTRA model to predict the user's intent behind the change text.
                                </li>
                            </ul>
                        </li>
                        <li><strong>Benefit vs. GBA Classification:</strong>
                            <ul>
                                <li>Built a custom classifier on top of the intent prediction to classify the change
                                    text into <strong>"Benefit"</strong> or <strong>"GBA"</strong>.</li>
                            </ul>
                        </li>
                        <li><strong>Entity Extraction using Rasa:</strong>
                            <ul>
                                <li>Integrated Rasa NLU to extract key entities such as dates, benefit types, GBA codes,
                                    and user-specific fields from the change text.</li>
                            </ul>
                        </li>
                        <li><strong>Evaluation & Optimization:</strong>
                            <ul>
                                <li>Used metrics like <strong>F1-score</strong>, <strong>Precision</strong>, and
                                    <strong>Recall</strong> to evaluate model performance.
                                </li>
                                <li>Addressed challenges like data imbalance, ambiguous text, and entity overlap through
                                    advanced techniques.</li>
                            </ul>
                        </li>
                    </ol>
                </div>

                <div>
                    <h3>Code Examples</h3>

                    <h4>1. Data Preprocessing</h4>
                    <pre><code>
                  import pandas as pd
                  from sklearn.model_selection import train_test_split
                  from transformers import ElectraTokenizer
                  
                  # Load FOBS change text data
                  data = pd.read_csv("fobs_change_text.csv")
                  
                  # Sample preprocessing
                  data['clean_text'] = data['change_text'].str.replace('[^a-zA-Z0-9 ]', '').str.lower()
                  
                  # Train-test split
                  train_texts, val_texts, train_labels, val_labels = train_test_split(
                      data['clean_text'], data['intent_label'], test_size=0.2, random_state=42
                  )
                  
                  # Tokenize using ELECTRA tokenizer
                  tokenizer = ElectraTokenizer.from_pretrained('google/electra-small-discriminator')
                  train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
                  val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)
                    </code></pre>

                    <h4>2. Fine-Tuning ELECTRA for Intent Prediction</h4>
                    <pre><code>
                  import torch
                  from transformers import ElectraForSequenceClassification, Trainer, TrainingArguments
                  
                  # Prepare ELECTRA model
                  model = ElectraForSequenceClassification.from_pretrained('google/electra-small-discriminator', num_labels=3)
                  
                  # Prepare dataset class
                  class FOBSDataset(torch.utils.data.Dataset):
                      def __init__(self, encodings, labels):
                          self.encodings = encodings
                          self.labels = labels
                  
                      def __getitem__(self, idx):
                          item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                          item['labels'] = torch.tensor(self.labels[idx])
                          return item
                  
                      def __len__(self):
                          return len(self.labels)
                  
                  # Create datasets
                  train_dataset = FOBSDataset(train_encodings, train_labels.tolist())
                  val_dataset = FOBSDataset(val_encodings, val_labels.tolist())
                  
                  # Training arguments
                  training_args = TrainingArguments(
                      output_dir='./results',
                      evaluation_strategy="epoch",
                      per_device_train_batch_size=16,
                      per_device_eval_batch_size=16,
                      num_train_epochs=4,
                      weight_decay=0.01,
                      logging_dir='./logs',
                  )
                  
                  # Trainer
                  trainer = Trainer(
                      model=model,
                      args=training_args,
                      train_dataset=train_dataset,
                      eval_dataset=val_dataset,
                  )
                  
                  # Fine-tune ELECTRA
                  trainer.train()
                    </code></pre>

                    <h4>3. Benefit vs. GBA Classification</h4>
                    <pre><code>
                  from sklearn.linear_model import LogisticRegression
                  from sklearn.metrics import classification_report
                  
                  # Get ELECTRA embeddings for the clean text
                  def get_embeddings(texts):
                      inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True)
                      with torch.no_grad():
                          outputs = model.electra(**inputs)
                      return outputs.last_hidden_state.mean(dim=1).numpy()
                  
                  # Generate embeddings
                  train_embeddings = get_embeddings(list(train_texts))
                  val_embeddings = get_embeddings(list(val_texts))
                  
                  # Train Logistic Regression classifier
                  clf = LogisticRegression(max_iter=1000)
                  clf.fit(train_embeddings, data.loc[train_texts.index, 'benefit_gba_label'])
                  
                  # Evaluate classifier
                  preds = clf.predict(val_embeddings)
                  print(classification_report(data.loc[val_texts.index, 'benefit_gba_label'], preds))
                    </code></pre>

                    <h4>4. Entity Extraction using Rasa</h4>
                    <pre><code>
                  # Sample Rasa NLU pipeline configuration (config.yml)
                  pipeline:
                    - name: WhitespaceTokenizer
                    - name: RegexFeaturizer
                    - name: CRFEntityExtractor
                    - name: DIETClassifier
                      epochs: 100
                    - name: EntitySynonymMapper
                  
                  # Sample NLU training data (nlu.yml)
                  nlu:
                  - intent: change_benefit
                    examples: |
                      - Change the benefit amount to $500 starting from [March 1st](start_date)
                      - Update the [GBA1234](gba_code) plan for user [John Doe](user_name)
                  
                  # Python Code to Run Entity Extraction
                  from rasa.nlu.model import Interpreter
                  
                  # Load trained Rasa model
                  interpreter = Interpreter.load("./models")
                  
                  # Parse new change text
                  message = "Please update benefit coverage starting from April 5th for user Sarah Connor."
                  result = interpreter.parse(message)
                  
                  # Output extracted entities
                  print("Intent:", result['intent']['name'])
                  print("Entities:", result['entities'])
                    </code></pre>

                    <h4>5. Evaluation Metrics</h4>
                    <pre><code>
                  from sklearn.metrics import accuracy_score, precision_recall_fscore_support
                  
                  # For ELECTRA intent prediction
                  intent_preds = trainer.predict(val_dataset)
                  intent_labels = intent_preds.label_ids
                  intent_predictions = intent_preds.predictions.argmax(axis=1)
                  
                  acc = accuracy_score(intent_labels, intent_predictions)
                  precision, recall, f1, _ = precision_recall_fscore_support(intent_labels, intent_predictions, average='weighted')
                  
                  print(f"Intent Prediction - Accuracy: {acc:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}")
                  
                  # For Benefit vs. GBA classification
                  print("Benefit vs. GBA Classification Report:")
                  print(classification_report(data.loc[val_texts.index, 'benefit_gba_label'], preds))
                    </code></pre>
                </div>

                <div>
                    <h3>Challenges & Solutions</h3>
                    <ul>
                        <li><strong>Ambiguous Change Texts:</strong><br>
                            <em>Solution:</em> Fine-tuned ELECTRA with domain-specific data to better handle ambiguous
                            intents.
                        </li>
                        <li><strong>Class Imbalance:</strong><br>
                            <em>Solution:</em> Used class weights in the Benefit/GBA classifier and applied data
                            augmentation for underrepresented classes.
                        </li>
                        <li><strong>Complex Entity Structures:</strong><br>
                            <em>Solution:</em> Leveraged Rasa's CRFEntityExtractor and DIETClassifier to accurately
                            extract overlapping entities and handle complex entity relationships.
                        </li>
                        <li><strong>Pipeline Integration:</strong><br>
                            <em>Solution:</em> Built a modular pipeline combining intent prediction, classification, and
                            entity extraction for seamless processing.
                        </li>
                    </ul>
                </div>

                <div>
                    <h3>Impact of the Project</h3>
                    <ul>
                        <li><strong>Accuracy Improvements:</strong> Intent prediction achieved <strong>~90%</strong>
                            accuracy, Benefit/GBA classification had an F1-score of <strong>~88%</strong>, and entity
                            extraction reached <strong>~85%</strong> accuracy.</li>
                        <li><strong>Efficiency Gains:</strong> The pipeline reduced processing time by
                            <strong>40%</strong>, leading to faster decision-making.
                        </li>
                        <li><strong>Enhanced Data Insights:</strong> Entity extraction provided granular data points for
                            downstream processes, improving overall data usability.</li>
                        <li><strong>Scalability:</strong> Designed the solution to be scalable, allowing integration
                            into existing FOBS workflows.</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Here’s a detailed explanation of the <strong>ELECTRA</strong> and <strong>Rasa</strong>
                    architectures, including how they work under the hood.</h2>
                <hr />

                <h3>⚡ <strong>ELECTRA Architecture (Efficiently Learning an Encoder that Classifies Token
                        Replacements Accurately)</strong></h3>
                <p>ELECTRA introduces a new pretraining method called <strong>Replaced Token Detection</strong> as
                    an alternative to traditional masked language modeling (like BERT).</p>

                <h4>🏛 <strong>Architecture Overview:</strong></h4>
                <p>ELECTRA consists of two main components:</p>
                <ol>
                    <li><strong>Generator</strong> (Small masked language model - MLM)</li>
                    <li><strong>Discriminator</strong> (Main model that detects replaced tokens)</li>
                </ol>
                <pre>
              [ Input Text ] → [ Generator (MLM) ] → [ Corrupted Text ] → [ Discriminator ] → [ Real or Fake Tokens? ]
                </pre>

                <hr />

                <h4>🔄 <strong>Step-by-Step Workflow:</strong></h4>
                <ol>
                    <li><strong>Input Processing:</strong>
                        <ul>
                            <li>The input text is tokenized and masked randomly (like BERT).</li>
                            <li>Example:
                                <pre>
              Original: The cat sat on the mat.
              Masked: The [MASK] sat on the mat.
                        </pre>
                            </li>
                        </ul>
                    </li>
                    <li><strong>Generator (Small MLM):</strong>
                        <ul>
                            <li>Predicts masked tokens (like BERT).</li>
                            <li>Example Output:
                                <pre>
              "The [dog] sat on the mat."
                        </pre>
                            </li>
                        </ul>
                    </li>
                    <li><strong>Corrupted Text Formation:</strong>
                        <ul>
                            <li>The predicted tokens from the generator are used to create a "corrupted" version of
                                the input.</li>
                            <li>Some tokens are replaced with generated tokens, others remain untouched.</li>
                        </ul>
                    </li>
                    <li><strong>Discriminator (Core ELECTRA Model):</strong>
                        <ul>
                            <li>The corrupted text is fed into the discriminator.</li>
                            <li>The discriminator predicts whether each token is <strong>Original</strong> or
                                <strong>Replaced</strong>.
                            </li>
                        </ul>
                    </li>
                    <li><strong>Loss Functions:</strong>
                        <ul>
                            <li><strong>Generator:</strong> Uses cross-entropy loss (same as BERT's MLM loss).</li>
                            <li><strong>Discriminator:</strong> Uses binary cross-entropy to predict real vs. fake
                                tokens.</li>
                        </ul>
                    </li>
                    <li><strong>Pretraining Objective - Replaced Token Detection:</strong>
                        <ul>
                            <li>ELECTRA focuses on teaching the discriminator to be highly sensitive to token
                                replacements.</li>
                        </ul>
                    </li>
                </ol>

                <hr />

                <h4>🛠 <strong>Advantages of ELECTRA:</strong></h4>
                <ul>
                    <li><strong>Efficiency:</strong> Trains faster than BERT while achieving similar or better
                        performance.</li>
                    <li><strong>Token-Level Discrimination:</strong> Learns from all tokens (not just masked ones),
                        leading to richer representations.</li>
                    <li><strong>Smaller Pretraining Cost:</strong> A smaller generator reduces compute needs.</li>
                </ul>

                <h4>🧑‍💻 <strong>Code Snippet - Using ELECTRA for Intent Classification</strong></h4>
                <pre><code class="language-python">
              from transformers import ElectraTokenizer, ElectraForSequenceClassification
              import torch
              
              # Load ELECTRA tokenizer and model
              tokenizer = ElectraTokenizer.from_pretrained("google/electra-small-discriminator")
              model = ElectraForSequenceClassification.from_pretrained("google/electra-small-discriminator", num_labels=3)
              
              # Sample text for intent prediction
              text = "Update the benefit coverage starting next month."
              inputs = tokenizer(text, return_tensors="pt")
              outputs = model(**inputs)
              
              # Get intent prediction
              predictions = torch.softmax(outputs.logits, dim=1)
              predicted_intent = torch.argmax(predictions, dim=1)
              print("Predicted Intent:", predicted_intent.item())
                </code></pre>

                <hr />

                <h3>🤖 <strong>Rasa Architecture (Open-Source Conversational AI)</strong></h3>
                <p>Rasa is a powerful open-source framework for building conversational agents, with two main
                    components:</p>
                <ol>
                    <li><strong>Rasa NLU</strong> (Natural Language Understanding) – Handles intent classification &
                        entity extraction.</li>
                    <li><strong>Rasa Core</strong> (Dialogue Management) – Manages conversation flow and responses.
                    </li>
                </ol>
                <pre>
              [ User Input ] → [ Rasa NLU ] → [ Parsed Intent & Entities ] → [ Rasa Core ] → [ Bot Response ]
                </pre>

                <hr />

                <h4>🏛 <strong>Rasa NLU Pipeline:</strong></h4>
                <p>Rasa NLU processes the raw user input and converts it into structured data.</p>
                <ol>
                    <li><strong>Tokenizer:</strong> Splits the text into tokens.</li>
                    <li><strong>Featurizer:</strong> Converts tokens into feature vectors.</li>
                    <li><strong>Intent Classification:</strong> Identifies the user's intent using
                        <strong>DIETClassifier</strong>.
                    </li>
                    <li><strong>Entity Extraction:</strong> Detects entities in the text.</li>
                </ol>

                <h4>🔍 <strong>Key Components:</strong></h4>
                <ul>
                    <li><strong>DIETClassifier:</strong> Multi-task transformer that predicts both intents and
                        entities.</li>
                    <li><strong>CRFEntityExtractor:</strong> Sequence labeling model that extracts entities based on
                        token-level features.</li>
                    <li><strong>RegexFeaturizer:</strong> Identifies predefined patterns (e.g., emails, phone
                        numbers).</li>
                </ul>

                <h4>🔄 <strong>Rasa Core - Dialogue Management:</strong></h4>
                <p>Uses a policy-based system to manage dialogue flow, tracking conversation state using
                    <strong>stories</strong> and guiding actions with policies like
                    <strong>MemoizationPolicy</strong> and <strong>TEDPolicy</strong>.
                </p>

                <h4>🧑‍💻 <strong>Code Snippet - Entity Extraction with Rasa</strong></h4>
                <pre><code class="language-yaml">
              nlu:
              - intent: change_benefit
                examples: |
                  - Change the benefit amount to $500 starting from [March 1st](start_date)
                  - Update the [GBA1234](gba_code) plan for user [John Doe](user_name)
                  - Modify coverage effective [April 5th](start_date)
                </code></pre>

                <pre><code class="language-yaml">
              pipeline:
                - name: WhitespaceTokenizer
                - name: RegexFeaturizer
                - name: DIETClassifier
                  epochs: 100
                - name: EntitySynonymMapper
                </code></pre>

                <pre><code class="language-python">
              from rasa.nlu.model import Interpreter
              
              # Load trained Rasa model
              interpreter = Interpreter.load("./models")
              
              # Parse user message
              message = "Please update benefit coverage starting from May 10th for user Sarah Connor."
              result = interpreter.parse(message)
              
              # Output
              print("Intent:", result['intent']['name'])
              print("Confidence:", result['intent']['confidence'])
              print("Entities:", result['entities'])
                </code></pre>

                <h4>🛠 <strong>Advantages of Rasa:</strong></h4>
                <ul>
                    <li><strong>Custom NLU Pipelines:</strong> Easily customizable for domain-specific needs.</li>
                    <li><strong>Multi-Task Learning:</strong> DIETClassifier handles both intents and entities.</li>
                    <li><strong>Open-Source & Extensible:</strong> Supports custom components and integrations.</li>
                    <li><strong>Modular Architecture:</strong> Can use only NLU or Core independently.</li>
                </ul>

                <h3>🚀 <strong>Combining ELECTRA + Rasa in Your Project:</strong></h3>
                <p><strong>ELECTRA</strong>: Handles <strong>intent prediction</strong> with fine-tuned
                    accuracy.<br />
                    <strong>Rasa</strong>: Focuses on <strong>entity extraction</strong> from the same input text.
                </p>
                <pre>
              [ User Input ]
                    ↓
              [ ELECTRA ] → Predict Intent
                    ↓
              [ Rasa NLU ] → Extract Entities
                    ↓
              [ Output ] → Intent + Entities for downstream processing
                </pre>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>📈 <strong>Time Series Analysis: A Detailed Explanation</strong></h2>
                <hr />

                <h3>⏳ <strong>What is Time Series Analysis?</strong></h3>
                <p>
                    Time Series Analysis involves analyzing data points collected or recorded at specific time
                    intervals.
                    The goal is to identify meaningful patterns, trends, and seasonalities in the data to make forecasts
                    and informed decisions.
                </p>

                <h3>🏛 <strong>Key Components of Time Series Data:</strong></h3>
                <ul>
                    <li><strong>Trend (T):</strong> Long-term upward or downward movement in the data.</li>
                    <li><strong>Seasonality (S):</strong> Repeating short-term cycles in the data (e.g., daily,
                        monthly).</li>
                    <li><strong>Cyclic Patterns (C):</strong> Long-term fluctuations not of a fixed period.</li>
                    <li><strong>Irregular/Noise (I):</strong> Random variations or anomalies that cannot be explained.
                    </li>
                </ul>

                <h3>🔄 <strong>Decomposition of Time Series:</strong></h3>
                <p>
                    A time series can be decomposed into its components using two models:
                </p>
                <ul>
                    <li><strong>Additive Model:</strong> Y(t) = T(t) + S(t) + C(t) + I(t)</li>
                    <li><strong>Multiplicative Model:</strong> Y(t) = T(t) * S(t) * C(t) * I(t)</li>
                </ul>

                <h3>📐 <strong>Mathematical Explanation:</strong></h3>
                <p>
                    Time series models often rely on autoregression and moving averages.
                </p>
                <ul>
                    <li><strong>Autoregressive (AR) Model:</strong> Y<sub>t</sub> = c + &phi;<sub>1</sub>Y<sub>t-1</sub>
                        + &phi;<sub>2</sub>Y<sub>t-2</sub> + ... + &phi;<sub>p</sub>Y<sub>t-p</sub> +
                        &epsilon;<sub>t</sub></li>
                    <li><strong>Moving Average (MA) Model:</strong> Y<sub>t</sub> = &mu; +
                        &theta;<sub>1</sub>&epsilon;<sub>t-1</sub> + &theta;<sub>2</sub>&epsilon;<sub>t-2</sub> + ... +
                        &theta;<sub>q</sub>&epsilon;<sub>t-q</sub> + &epsilon;<sub>t</sub></li>
                    <li><strong>ARIMA Model:</strong> Combines AR and MA with differencing (I) to make data stationary.
                        <br />
                        ARIMA(p, d, q): p = AR terms, d = differencing order, q = MA terms.
                    </li>
                </ul>

                <h3>📊 <strong>Key Techniques in Time Series Analysis:</strong></h3>
                <ul>
                    <li><strong>Stationarity Check:</strong> Ensures constant mean and variance over time (e.g., ADF
                        Test).</li>
                    <li><strong>Autocorrelation & Partial Autocorrelation:</strong> Helps determine lags for AR/MA
                        models.</li>
                    <li><strong>Seasonal Decomposition:</strong> Separates trend, seasonality, and noise.</li>
                    <li><strong>Cross-Correlation:</strong> Identifies relationships between two time series.</li>
                </ul>

                <h3>🧑‍💻 <strong>Code Snippet - Basic Time Series Analysis Using Python</strong></h3>
                <pre><code class="language-python">
              import pandas as pd
              import matplotlib.pyplot as plt
              from statsmodels.tsa.seasonal import seasonal_decompose
              
              # Load time series data
              data = pd.read_csv('timeseries.csv', parse_dates=['date'], index_col='date')
              
              # Plot the time series
              data.plot(title='Time Series Data')
              plt.xlabel('Date')
              plt.ylabel('Value')
              plt.show()
              
              # Decompose the time series
              result = seasonal_decompose(data['value'], model='additive')
              
              # Plot decomposition
              result.plot()
              plt.show()
                </code></pre>

                <h3>📉 <strong>Graphical Representation:</strong></h3>
                <p>
                    The above code will produce two plots:
                </p>
                <ul>
                    <li><strong>Original Time Series:</strong> Displays the raw data over time.</li>
                    <li><strong>Decomposition Plot:</strong> Separates the data into Trend, Seasonality, and Residuals.
                    </li>
                </ul>

                <h3>🚀 <strong>Advanced Techniques:</strong></h3>
                <ul>
                    <li><strong>LSTM (Long Short-Term Memory):</strong> Recurrent neural networks for sequential data.
                    </li>
                    <li><strong>GRU (Gated Recurrent Units):</strong> A lighter version of LSTM.</li>
                    <li><strong>Transformer Models:</strong> Applied for time series forecasting (e.g., Time Series
                        Transformer).</li>
                </ul>

                <h3>✅ <strong>Applications of Time Series Analysis:</strong></h3>
                <ul>
                    <li>Stock Market Prediction 📉📈</li>
                    <li>Weather Forecasting 🌦</li>
                    <li>Sales Forecasting 🛍</li>
                    <li>Healthcare Monitoring 🏥</li>
                    <li>Anomaly Detection 🔍</li>
                </ul>

                <h3>💡 <strong>Best Practices:</strong></h3>
                <ul>
                    <li>Ensure data quality and handle missing values.</li>
                    <li>Use rolling statistics for stationarity checks.</li>
                    <li>Always validate the model using out-of-sample testing.</li>
                </ul>

                <p>
                    Would you like a deeper dive into <strong>ARIMA</strong> modeling or advanced
                    <strong>LSTM</strong>-based forecasting techniques? 🚀
                </p>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>




</body>

</html>