<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Projects</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- MathJax for Equations -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

    <style>
        /* Global Styles */
        body {
            font-family: 'Courier New', monospace;
            margin: 0;
            padding: 0;
            color: #ffffff;
            background: url('../assets/images/ml-background.jpg') no-repeat center center fixed;
            background-size: cover;
            position: relative;
        }

        /* Dark Overlay */
        body::before {
            content: "";
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1;
        }

        /* Layout Containers */
        header,
        footer {
            background: linear-gradient(135deg, #0829df, #04013c);
            text-align: center;
            padding: 20px;
            font-size: 24px;
            position: relative;
            z-index: 10;
        }

        main {
            max-width: 900px;
            margin: 40px auto;
            padding: 25px;
            background: rgba(19, 50, 185, 0.85);
            border-radius: 10px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            position: relative;
            z-index: 10;
        }

        h1,
        h2,
        h3 {
            color: #d9dcdf;
        }

        /* Styling for Key Sections */
        .case h3 {
            color: #ffcc00;
        }

        .case p {
            color: #ffffff;
        }

        /* Table Styles */
        table {
            font-family: 'Courier New', monospace;
            width: 100%;
            border-collapse: collapse;
            margin: 10px 0;
            background: rgba(255, 255, 255, 0.1);
        }

        th,
        td {
            border: 1px solid #ccc;
            padding: 8px;
            text-align: center;
            font-size: 14px;
        }

        th {
            background: rgba(0, 0, 0, 0.2);
        }

        /* Image Styling */
        .image-container {
            text-align: center;
            margin: 10px 0;
        }

        .image-container img {
            width: 35%;
            border-radius: 5px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
        }

        /* Content Alignment */
        .content-container {
            display: flex;
            align-items: center;
            justify-content: space-between;
            gap: 10px;
        }

        .image-container {
            flex: 1;
        }

        .image-container img {
            width: 65%;
            border-radius: 10px;
        }

        .text-container {
            flex: 1;
            width: 55%;
        }

        /* Failure Cases Section */
        .failure-cases {
            width: 90%;
            max-width: 1600px;
            background: rgba(255, 255, 255, 0.1);
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
            margin: 20px auto;
            text-align: justify;
        }

        .case {
            width: 100%;
            background: rgba(255, 255, 255, 0.15);
            padding: 15px;
            margin-bottom: 15px;
            border-radius: 6px;
        }

        /* Decision Tree Node Styles */
        .node circle {
            fill: #69b3a2;
            stroke: #555;
            stroke-width: 2px;
        }

        .node text {
            font-size: 14px;
            font-family: 'Courier New', monospace;
            fill: white;
        }

        .link {
            fill: none;
            stroke: #ccc;
            stroke-width: 2px;
        }
    </style>
</head>

<header>
    <h1>Projects Worked On </h1>
</header>

<body>
    <div class="failure-cases">
        <div class="case">
            <div style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                <div>
                    <h2>Project Overview: Building an AI-Powered Knowledge Repository and Search Engine for Enterprise
                        Code and Documents</h2>
                    <p>This project aims to create a centralized repository for enterprise code and documents, enabling
                        efficient search and retrieval of information using AI-powered semantic understanding and
                        analysis. The system will leverage various AI/ML techniques, including Natural Language
                        Processing (NLP), Optical Character Recognition (OCR), and potentially machine learning models
                        for classification and entity recognition.</p>

                    <h3>Key Components and Functionality:</h3>
                    <ul>
                        <li><strong>Data Ingestion and Processing:</strong>
                            <ul>
                                <li><strong>1.0 Crawler & Delta:</strong> This component is responsible for collecting
                                    documents from various sources like SharePoint and S3 buckets. The "Delta" aspect
                                    suggests it focuses on incremental updates and changes.</li>
                                <li><strong>Nightly Document Delta Job Scheduler:</strong> This schedules the crawling
                                    and updating process.</li>
                                <li><strong>SOT Doc Pipeline & Delta:</strong> This pipeline processes the ingested
                                    documents, likely involving steps like format conversion, metadata extraction, and
                                    text preprocessing.</li>
                                <li><strong>Data Extraction/OCR:</strong> Components like Textract and openpyxl are used
                                    to extract text from different document formats, including OCR for images or scanned
                                    documents.</li>
                                <li><strong>Snippet Creation:</strong> This step likely involves breaking down documents
                                    into smaller, searchable chunks or snippets.</li>
                            </ul>
                        </li>

                        <li><strong>AI-Driven Understanding and Enrichment:</strong>
                            <ul>
                                <li><strong>2.0 Doc Classifier:</strong> This component classifies documents using the
                                    "2.0 Docs" classification model, potentially using techniques like deep learning or
                                    traditional machine learning.</li>
                                <li><strong>Custom NER (Named Entity Recognition):</strong> This component identifies
                                    and extracts key entities (e.g., people, organizations, locations) from the text.
                                </li>
                                <li><strong>Recognize Entities:</strong> Similar to NER, this focuses on identifying and
                                    extracting specific entities relevant to the domain.</li>
                                <li><strong>LLM (Large Language Model):</strong> This likely plays a crucial role in
                                    understanding the semantics of the text, calculating coverage and limits, and
                                    generating the final "SOT" (Source of Truth) data model.</li>
                                <li><strong>Classification/Semantic Model (FAISS):</strong> FAISS (Facebook AI
                                    Similarity Search) is used for efficient similarity search and clustering of
                                    documents based on their semantic meaning.</li>
                            </ul>
                        </li>

                        <li><strong>Data Storage and Management:</strong>
                            <ul>
                                <li><strong>EFS (Elastic File System):</strong> Provides temporary storage during
                                    processing.</li>
                                <li><strong>Mongo DB (Metadata Storage):</strong> Stores document metadata and revision
                                    history.</li>
                                <li><strong>S3 Bucket (Documents):</strong> Stores the actual document files.</li>
                                <li><strong>Knowledge Graph (Mongo DB):</strong> Stores relationships between entities
                                    and concepts extracted from the documents.</li>
                                <li><strong>Facets Data Model (Mongo DB):</strong> Stores data related to facets
                                    (categories or filters) for search and navigation.</li>
                                <li><strong>MySQL Aurora (Facets Staging):</strong> A relational database used for
                                    staging facet data before it's finalized.</li>
                            </ul>
                        </li>

                        <li><strong>Search and Retrieval:</strong>
                            <ul>
                                <li><strong>Amazon Kendra:</strong> An enterprise search service that likely integrates
                                    with the system to provide indexing and search capabilities.</li>
                                <li><strong>Kendra 1.0 Search:</strong> Represents an earlier version of the search
                                    functionality.</li>
                                <li><strong>Search Engine (UI & APIs):</strong> Provides the user interface and APIs for
                                    searching the repository.</li>
                                <li><strong>Facets Code, Coverage, Limits:</strong> These components relate to specific
                                    search facets, enabling users to filter results based on code details, coverage, or
                                    limits.</li>
                                <li><strong>Mapping Engine:</strong> Likely used to map search queries to relevant
                                    documents or concepts in the knowledge graph.</li>
                            </ul>
                        </li>

                        <li><strong>User Interface and Access:</strong>
                            <ul>
                                <li><strong>Users (GURU UI):</strong> Users interact with the system through a dedicated
                                    UI to search for documents, code snippets, or information.</li>
                            </ul>
                        </li>
                    </ul>

                    <h3>AI/ML Focus:</h3>
                    <ul>
                        <li><strong>Semantic Search:</strong> Utilizing LLMs and FAISS to enable search based on the
                            meaning of the query rather than just keywords.</li>
                        <li><strong>Document Classification:</strong> Automatically categorizing documents for easier
                            organization and retrieval.</li>
                        <li><strong>Named Entity Recognition:</strong> Extracting key entities to enrich the data and
                            enable faceted search.</li>
                        <li><strong>Knowledge Graph Construction:</strong> Building a knowledge graph to capture
                            relationships between concepts and improve search relevance.</li>
                    </ul>

                    <h3>Potential Use Cases:</h3>
                    <ul>
                        <li>Finding relevant code examples or documentation.</li>
                        <li>Searching for information related to specific projects or topics.</li>
                        <li>Identifying experts on particular subjects.</li>
                        <li>Understanding the relationships between different concepts in the enterprise knowledge base.
                        </li>
                    </ul>

                    <h3>Challenges and Considerations:</h3>
                    <ul>
                        <li><strong>Data Quality and Consistency:</strong> Ensuring the accuracy and consistency of data
                            from various sources.</li>
                        <li><strong>Scalability:</strong> Designing the system to handle a large volume of documents and
                            user requests.</li>
                        <li><strong>Performance:</strong> Optimizing the search and retrieval process for speed and
                            efficiency.</li>
                        <li><strong>Security:</strong> Protecting sensitive information within the repository.</li>
                    </ul>

                    <p>Overall, this project aims to build a powerful AI-driven knowledge management system that
                        empowers users to efficiently access and leverage enterprise information assets. Let me know if
                        you'd like me to elaborate on any specific component or aspect of this project!</p>
                </div>

            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Project Overview: Building an AI-Powered Knowledge Repository and Search Engine for Enterprise Code
                    and Documents</h2>
                <p>In this project, I worked as an AI engineer where my main focus was on implementing **Named Entity
                    Recognition (NER)** and **document classification** to help build a centralized repository for
                    enterprise code and documents. The goal was to enable efficient search and retrieval of information
                    using AI-powered semantic understanding and analysis. We utilized a variety of AI/ML techniques,
                    including NLP and machine learning models for entity recognition and document classification.</p>

                <h3>Key Responsibilities and Contributions:</h3>
                <ul>
                    <li><strong>Named Entity Recognition (NER):</strong>
                        <p>One of my core contributions was developing the NER model to extract key entities such as
                            people, organizations, and locations from the documents. This involved fine-tuning existing
                            NER models to suit the specific requirements of our enterprise domain. The goal was to
                            enhance the document’s semantic structure and make it easier for users to retrieve the most
                            relevant information based on entities.</p>
                        <p>We used pre-trained models like spaCy and fine-tuned them with domain-specific data to
                            improve accuracy. I also worked on integrating this NER model with the broader pipeline to
                            tag documents with entities, enabling the search engine to understand and filter results
                            based on recognized entities.</p>
                    </li>

                    <li><strong>Document Classification:</strong>
                        <p>Another key area of my work was **document classification**. We built a model to categorize
                            incoming documents into predefined classes. This made it easier to organize and retrieve
                            documents based on their content, significantly improving the user experience. I implemented
                            classification models using deep learning techniques and also explored traditional machine
                            learning algorithms.</p>
                        <p>For classification, we leveraged techniques such as LSTM (Long Short-Term Memory) networks
                            and BERT (Bidirectional Encoder Representations from Transformers). These models helped us
                            classify documents accurately based on their semantic meaning. Fine-tuning BERT on our
                            dataset allowed for better generalization across different types of documents in the
                            enterprise repository.</p>
                    </li>

                    <li><strong>AI-Driven Understanding and Enrichment:</strong>
                        <p>In addition to NER and classification, I collaborated on integrating the **Large Language
                            Model (LLM)** into our workflow. The LLM helped us understand the semantics of documents,
                            calculate coverage, and refine the final "Source of Truth" data model.</p>
                        <p>By using a combination of AI-driven enrichment and semantic search models, I was able to help
                            the system interpret complex user queries and retrieve highly relevant documents from the
                            knowledge base.</p>
                    </li>
                </ul>

                <h3>AI/ML Focus:</h3>
                <ul>
                    <li><strong>Semantic Search:</strong> Leveraged LLMs and FAISS (Facebook AI Similarity Search) to
                        improve the search experience by focusing on the meaning of the query rather than just keywords.
                    </li>
                    <li><strong>Named Entity Recognition:</strong> My work in NER helped to identify and enrich
                        documents with key entities, improving the accuracy of document retrieval and enabling advanced
                        search functionalities.</li>
                    <li><strong>Document Classification:</strong> I developed the classification system that allowed
                        documents to be organized based on their content, which streamlined the process of finding
                        relevant information.</li>
                    <li><strong>Knowledge Graph Construction:</strong> I also contributed to building the knowledge
                        graph that captured relationships between entities and concepts within the documents, enhancing
                        the search relevance.</li>
                </ul>

                <h3>Challenges and Problem-Solving:</h3>
                <ul>
                    <li><strong>Data Quality:</strong> A challenge was ensuring that the data fed into the NER and
                        classification models was high quality. I worked closely with the data engineering team to clean
                        and preprocess the text data, which significantly improved the model's performance.</li>
                    <li><strong>Model Generalization:</strong> Fine-tuning the models to generalize across various types
                        of documents in the enterprise repository was another challenge. To address this, I used
                        transfer learning techniques to adapt pre-trained models to our specific domain.</li>
                    <li><strong>Scalability:</strong> The solution needed to handle a large volume of documents and
                        queries. I collaborated with the infrastructure team to ensure the AI models were optimized for
                        performance and scalability, enabling fast and accurate retrieval.</li>
                </ul>

                <h3>Impact and Results:</h3>
                <p>My contributions to the NER and document classification components had a significant impact on the
                    overall performance of the knowledge repository. With more accurate entity extraction and better
                    document categorization, the system became much more efficient in serving user queries. The
                    AI-powered search system was able to understand the semantic context of queries, leading to better
                    search results and user satisfaction.</p>
                <p>Overall, this project helped build a robust, AI-driven knowledge management system that greatly
                    improved the accessibility and usefulness of enterprise documents.</p>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="healthcare-ai-project">
                <h2>Agentic Bot Framework for Healthcare Data Analysis</h2>

                <div class="project-overview">
                    <h3>Project Overview</h3>
                    <p>A sophisticated framework combining LLMs and specialized agents to analyze Elevance Health data
                        through structured workflows, enabling comprehensive healthcare data analysis and natural
                        language query processing.</p>
                </div>

                <div class="architecture-components">
                    <h3>Architectural Components</h3>

                    <div class="component">
                        <h4>Core Modules</h4>
                        <ul>
                            <li><strong>GenAI Chat API</strong>: Natural language interface powered by LLMs</li>
                            <li><strong>Agentic Controller</strong>: Central orchestration module</li>
                            <li><strong>LLM Gateway</strong>: Interface for model interactions</li>
                        </ul>
                    </div>

                    <div class="component">
                        <h4>Specialized Agents</h4>
                        <ul>
                            <li><strong>Benefit Agent</strong>: Insurance & coverage data processor</li>
                            <li><strong>Jira Agent</strong>: Workflow integration manager</li>
                            <li><strong>Future Agent</strong>: Modular expansion slot</li>
                        </ul>
                    </div>
                </div>

                <div class="data-workflow">
                    <h3>Data Processing Pipeline</h3>
                    <ol>
                        <li>User query via GenAI Chat API</li>
                        <li>Intent parsing by Agentic Controller</li>
                        <li>Task delegation to specialized agents</li>
                        <li>Multi-source data retrieval:
                            <ul>
                                <li>Elevance Health Data (PHI)</li>
                                <li>Fobs MongoDB (unstructured)</li>
                                <li>Oracle Stats (aggregated)</li>
                            </ul>
                        </li>
                        <li>LLM-powered synthesis & response</li>
                    </ol>
                </div>

                <div class="tech-specs">
                    <h3>Technical Specifications</h3>
                    <div class="spec-section">
                        <h4>Technology Stack</h4>
                        <ul>
                            <li>Python-based LLM frameworks</li>
                            <li>MongoDB + Oracle hybrid database</li>
                            <li>LangChain agent architecture</li>
                        </ul>
                    </div>

                    <div class="spec-section">
                        <h4>Key Capabilities</h4>
                        <ul>
                            <li>Natural language query resolution</li>
                            <li>Multi-agent task coordination</li>
                            <li>Context-aware session management</li>
                        </ul>
                    </div>
                </div>

                <div class="compliance-security">
                    <h3>Security Implementation</h3>
                    <ul>
                        <li>HIPAA-compliant data handling</li>
                        <li>PHI anonymization layers</li>
                        <li>Enterprise-grade encryption</li>
                    </ul>
                </div>

                <div class="performance-metrics">
                    <h3>System Performance</h3>
                    <div class="metric-grid">
                        <div class="metric">
                            <span class="value">98%</span>
                            <span class="label">Query Accuracy</span>
                        </div>
                        <div class="metric">
                            <span class="value">2.4s</span>
                            <span class="label">Avg Response Time</span>
                        </div>
                        <div class="metric">
                            <span class="value">50+</span>
                            <span class="label">Simultaneous Agents</span>
                        </div>
                    </div>
                </div>

                <div class="challenges">
                    <h3>Technical Challenges Overcome</h3>
                    <ul>
                        <li>LLM hallucination mitigation</li>
                        <li>Multi-database synchronization</li>
                        <li>Real-time PHI processing</li>
                    </ul>
                </div>
            </div>

            <style>
                .healthcare-ai-project {
                    max-width: 1200px;
                    margin: 2rem auto;
                    padding: 2rem;
                    border-radius: 10px;
                    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                }

                .component {
                    padding: 1.5rem;
                    margin: 1rem 0;
                    border-radius: 8px;
                    border-left: 4px solid #004d40;
                }

                .metric-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
                    gap: 1rem;
                }

                .metric {
                    text-align: center;
                    padding: 1rem;
                    background: white;
                    border-radius: 8px;
                }

                .value {
                    display: block;
                    font-size: 2rem;
                    font-weight: bold;
                    color: #004d40;
                }

                .label {
                    color: #666;
                    font-size: 0.9rem;
                }

                h3 {
                    color: #004d40;
                    border-bottom: 2px solid #00bfa5;
                    padding-bottom: 0.5rem;
                }
            </style>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Electricity Demand Forecasting for London</h2>

                <h3>Introduction</h3>
                <p>Accurate forecasts for electricity demand are important for electric utilities and regional
                    transmission organizations. Long term forecasts can reduce investment risk, medium term forecasts
                    help in planning fuel purchases and scheduling plant maintenance, and short term forecasts are
                    essential in matching electricity generation and demand for grid reliability.

                    This project focused on medium term forecasting, specifically making one-week-out forecasts at a
                    resolution of one hour for London. Historical electricity demand and weather data from 2002-2020 is
                    used. A combination of timeseries analysis and regression models are used to make forecasting
                    predictions, which are compared against the observed hourly electricity demand for a series of
                    one-week intervals.
                <h3>Data Collection</h3>

                <ul>
                    <p>Electricity Demand Data</p>
                    <li>County of London Electric Supply Company(C.L.E.S.Co) Limited provides historical
                        electricity demand data. The demand data for each year is segmented by weather zones, as seen in
                        the
                        schematic below.

                        At first we performed some basic processing on the raw data, such as converting timestamps to
                        datetime indexes, removing duplicate entries (by taking an average of values with the same
                        timestamp), and filling missing values (by backfilling from the next available entry).

                        So afterpreprocessing we got a dataframe of time and load</li>



                    <p>Weather Data</p>
                    <li>Next from domain knowledge we get to know that weather features like temp, humidity had an
                        impact on electricity consumption. So to analyse load we need weatheric data.

                        After searching for a while and as suggested by client we tried to get the weather information
                        from Weather Underground. Next, we built a parser to extract relevant data from a single API
                        endpoint. Finally, the data scraping script is extended to page through all dates of interest to
                        compile hourly weather data from 2002-2020 into a single dataframe.

                        Through a series of wrapper functions we collected data from individual webpages for entire span
                        of 2002-2020 and processed the data into a nice csv format to get time, temp, hum, wind_spd &
                        cond.

                        The following code are a series of wrapper functions and helper functions which iteratively
                        collect data from individual webpages for the entire span from 2002-2017, process this data into
                        the a nice format, and save all data to a single CSV file for later use. Some notes on data
                        processing:

                        Most wind speed data is numeric, but some is missing or simply "calm". These are converted to
                        the mean and lowest observed wind speed, respectively.
                        Weather condition is a categorical feature. The string representation from the API is converted
                        to an integer.
                        Humidity values are converted to percentages on the range [0, 1].
                        Hourly frequency of the data is enformced (i.e. duplicate entries are removed).</li>
                    </li>
                </ul>
                </p>

                <h3>Feature Engineering</h3>
                <p>First, the categorical feature "condtion" is converted into a one-hot encoding. The original dataset
                    from Weather Underground uses a convention of 17 different weather condtions, many of which are
                    quite similar. The 17 original categories are grouped into 6 broad categories, as described below:

                    Weather Group Original Conditions
                    clear clear
                    cloud scattered clouds, partly cloudy, mostly cloudy, overcast
                    rain rain, light rain, heavy rain
                    tstorm thunderstorm, thunderstorms and rain, light thunderstorm and rain, heavy thunderstorm and
                    rain
                    fog fog, haze, mist
                    snow light snow
                    Next, the electricity demand and weather data are merged into a single dataframe. Missing values
                    created by the merge need to be filled in: for the one-hot encoding features back-filling is used,
                    for the continuous features interpolation is used.

                    Next, we created some more temporal features. For instance, whether it is a weekday or not likely
                    has an impact on electricity consumption, so a boolean feature "weekday" is created. Similarly, the
                    time of day certainly has a large impact on electricity consumption. Even though this is a time
                    series problem, in the next section the model in this project is decomposed into two parts. To
                    capture time of day effects in the second part an explicit feature must be created. Since time is a
                    cyclic variable, a sine/cosine transformation is used on the hour the day.

                    sin_theta = np.sin(2*np.pi*hour.values/24)
                    cosine_theta = np.cos(2*np.pi*hour.values/24)
                    Finally, continuous features must be normalized, as models are generally easier to train if all
                    features are on roughly the same scale. The simplest method for normalizing is to simply transform
                    each continous feature to be on the range [0, 1]. Note that the minimum and maximum values for each
                    feature are saved in order to transform them back to their original scale later.</p>

                <h3>Designing Models</h3>
                <p>In general, time series forecasting becomes quite difficult as the period of forecasting decreases.
                    Therefore, this project decomposes the original electricity demand timeseries into two components.
                    The first component is the monthly average electricity demand, which is modeled using traditional
                    timeseries methods. The second component is what this project will refer to as the "hourly
                    residuals". In other words, after removing the monthly averages from the orignal timeseries, what is
                    left over is the amount above or below the monthly average at each hour (hence, hourly residual).
                    The hourly residuals are modeled using various regression models, and are what the engineered
                    features are used to help predict. After building two models (monthly and hourly), the predictions
                    from each can be added together to get a full prediction for the electrcity demand at hourly
                    frequency.

                </p>
                **Summary: Electricity Demand Forecasting Models**

                ### **Monthly Averages (SARIMA)**
                - **Model**: SARIMA (seasonal ARIMA) captures **yearly seasonality** (`m=12`) and trends in monthly
                demand.
                - **Data Split**: Sequential training (`2002–2014`), validation (`2015–2016`), test (`2017–2020`) sets.
                - **Tuning**: Grid search for hyperparameters (values 0/1) using **MAE** (to avoid issues with
                normalized MAPE).
                - **Insights**:
                - Clear **upward trend** (population growth) and **seasonality** (summer peaks, smaller winter spikes).
                - Winter peaks correlate with holidays/cold weather.

                ---

                ### **Hourly Residuals (Regression)**
                - **Approach**: Residuals (actual – monthly average) modeled via regression for **1-week-ahead hourly
                forecasts**.
                - **Baseline**: Persistence model (last week’s value as forecast).
                - **Models Tested**:
                1. **Linear Regression**: Outperformed baseline but lacked nuance (overweighted time-of-day features).
                2. **Gradient Boosting (GBR)**: Best performance (61% MAPE improvement), captured daily peaks.
                3. **MLP**: Better than linear regression but lagged GBR due to hyperparameter tuning challenges.
                - **Validation**: Weekly forecasts concatenated to avoid error snowballing.

                ---

                ### **Key Takeaways**
                - **SARIMA + GBR combo** delivered optimal accuracy.
                - **Future Improvements**: Deeper hyperparameter tuning (especially MLP), adding contextual features
                (e.g., events).

                <h3>Evaluating Final Models</h3>
                <p>The predictions from both the monthly and hourly models are combined and evaluated against the test
                    data. The first block of code gets all of the required forecast predictions on the test set for each
                    model. The second block of code adds the hourly and monthly models to get the full electricity
                    demand prediction, and also unnormalizes these predictions back to the scale of the original
                    dataset.

                    With the full predictions for each model, the performance of each of them can now be evaluated. The
                    MAPE is computed for each model below.

                    All three models perform better than the baseline. The GBR performs best, followed by MLP, and
                    linear regression is significantly worse than both of these. To visualize the forecasting
                    predictions, below are plots of one-week-out forecasts for each model over a span of two weeks on
                    the test set.

                    In agreement with the MAPE results, GBR clearly has the best performance, followed by MLP. Linear
                    regression performs quite poorly, making almost no attempt to follow the weekly trend as it rises
                    and falls. For completeness, the GBR model predictions are plotted below for the full time span of
                    the test data.</p>

                <h3>Summary & Future Work</h3>
                <p>This project considered the issue of forecasting electricity demand at one week intervals with a
                    resolution of one hour. This is an important application for utilities and power plant operators so
                    that they can make informed fuel purchase decisions and schedule maintenece at optimal times. In
                    order to make more accurate forecasts, the original timeseries data was decomposed into monthly
                    averages and hourly residuals. The monthly averages were modeled using a SARIMA model, and the
                    hourly residuals were modeled using three different regression models. Of these, gradient boosting
                    regression showed the best performance on the test set, with a 61% improvement over the baseline
                    persistance model.

                    The accuracy of the best model (gradient boosting regression) is actually quite good. This model
                    could certainly be used for one-week-out forecasting, at least as a first-order estimate. If a
                    higher accuracy model is required, the authors present two suggestions for further study:

                    For both GBR and MLP, only a very rough grid search over hyperparameters was performed, and the
                    possible values of hyperparameters were chosen somewhat arbitrarily. A more thorough investigation
                    of hyperparameters is strongly suggested, especially for MLP.
                    The most obvious features for predicting electricity demand were used in this project:
                    autoregressive features, temporal features, and weather features. A study of where the model is
                    predicting with the lowest accuracies could be interesting, with the hope of finding a pattern which
                    could be fixed with additional features. For example, if one found that the model was consistently
                    under-predicting electricity demand during major sporting events, adding a boolean feature of
                    "major_event" could help improve model accuracy.</p>
            </div>


        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
        </div>
    </div>




</body>

</html>