<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Learning</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;700&display=swap" rel="stylesheet">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">

    <!-- MathJax for Equations -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

  <style>
    /* Global Styles */
    body {
      font-family: 'Courier New', monospace;
      margin: 0;
      padding: 0;
      color: #ced0d1; /* Changed font color to black */
      background: url('../assets/images/Background.jpg') no-repeat center center fixed;
      background-size: cover;
      position: relative;
    }

    /* Dark Overlay */
    body::before {
      content: "";
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.5);
      z-index: 1;
    }

    /* Layout Containers */
    header,
    footer {
      background: linear-gradient(135deg, #002c26, #004d40);
      text-align: center;
      padding: 20px;
      font-size: 24px;
      position: relative;
      z-index: 10;
    }

    main {
      max-width: 900px;
      margin: 40px auto;
      padding: 25px;
      background: rgba(116, 124, 123, 0.85);
      border-radius: 10px;
      box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
      position: relative;
      z-index: 10;
    }

    h2,
    h3 {
      color: #e9e63b; /* Changed text color to black */
    }

    table {
      width: 100%;
      border-collapse: collapse;
      margin: 20px 0;
      background: rgba(255, 255, 255, 0.1);
    }

    th,
    td {
      border: 1px solid rgba(255, 255, 255, 0.43);
      padding: 12px;
      text-align: center;
      color: #000000; /* Changed text color to black */
    }

    th {
      background: rgba(255, 255, 255, 0.2);
      font-weight: bold;
    }

    .image-container {
      text-align: center;
      margin: 30px 0;
    }

    .image-container img {
      width: 85%;
      border-radius: 10px;
      box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
    }

    .result {
      background: rgba(255, 255, 255, 0.2);
      padding: 15px;
      margin: 10px 0;
      border-radius: 8px;
      font-weight: bold;
    }

    .failure-cases {
      width: 90%;
      max-width: 1600px;
      background: rgba(255, 255, 255, 0.1);
      padding: 20px;
      border-radius: 8px;
      box-shadow: 0px 5px 10px rgba(0, 0, 0, 0.2);
      margin: 20px auto;
      display: block;
      text-align: justify;
    }

    .case {
      width: 100%;
      background: rgba(7, 7, 7, 0.15);
      padding: 15px;
      margin-bottom: 15px;
      border-radius: 6px;
    }

    .case h3 {
      color: #000000; /* Changed text color to black */
      margin-bottom: 5px;
    }

    .case p {
      color: #000000; /* Changed text color to black */
    }
    li{
      color: #000000;
    }
    body::before {
      pointer-events: none;
    }
  </style>
</head>

<header>
    <h1>Deep Learning </h1>
</header>

<body>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Logistic Regression vs. Perceptron</h2>

                <h3>1. Geometric Objective</h3>
                <p>Both algorithms aim to find a <strong>hyperplane</strong> that separates two classes in the feature
                    space.</p>

                <h3>2. Logistic Regression</h3>
                <ul>
                    <li><strong>Prediction Function</strong>:
                        \[
                        \hat{y}_i = \sigma(W^T x_i + b) \quad \text{where } \sigma(z) = \frac{1}{1 + e^{-z}} \text{
                        (sigmoid)}
                        \]
                    </li>
                    <li><strong>Learning</strong>: Adjusts weights (\(W\)) and bias (\(b\)) using maximum likelihood
                        estimation.</li>
                    <li><strong>Output</strong>: Probabilistic (values between 0 and 1).</li>
                </ul>

                <h3>3. Perceptron</h3>
                <ul>
                    <li><strong>Prediction Function</strong>:
                        \[
                        f(x) = \sum_{j=0}^d w_j x_{ij} + b
                        \]
                        <em>Activation</em>: Step function (e.g., \(1\) if \(f(x) > 0\), else \(0\)).
                    </li>
                    <li><strong>Learning Rule</strong>:
                        <ul>
                            <li>If misclassified, update weights:
                                \[
                                w_j \leftarrow w_j + \eta (y_i - \hat{y}_i) x_{ij}
                                \]
                            </li>
                            <li>\(\eta\): Learning rate; \(y_i\): True label; \(\hat{y}_i\): Predicted label.</li>
                        </ul>
                    </li>
                    <li><strong>Output</strong>: Binary (0 or 1).</li>
                </ul>

                <h3>4. Key Differences</h3>
                <table border="1">
                    <tr>
                        <th>Aspect</th>
                        <th>Logistic Regression</th>
                        <th>Perceptron</th>
                    </tr>
                    <tr>
                        <td><strong>Activation</strong></td>
                        <td>Sigmoid (probabilistic)</td>
                        <td>Step function (deterministic)</td>
                    </tr>
                    <tr>
                        <td><strong>Learning</strong></td>
                        <td>Optimizes via gradient descent</td>
                        <td>Updates weights on misclassification</td>
                    </tr>
                    <tr>
                        <td><strong>Use Case</strong></td>
                        <td>Probabilistic classification</td>
                        <td>Linearly separable binary classification</td>
                    </tr>
                </table>

                <h3>5. Graphical Representation (Conceptual)</h3>
                <div style="text-align: center;">
                    <img src="https://i.imgur.com/ABC123.png" alt="Hyperplane Separation" width="400">
                    <p><em>Both algorithms seek a hyperplane (line in 2D) to separate classes.</em></p>
                </div>

                <h3>6. Limitations</h3>
                <ul>
                    <li><strong>Logistic Regression</strong>: Requires probabilistic interpretation; not ideal for
                        non-linear data.</li>
                    <li><strong>Perceptron</strong>: Only works for linearly separable data; no probabilistic output.
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Multi-Layer Perceptron (MLP)</h2>

                <div style="display: flex; gap: 30px; align-items: flex-start;">
                    <!-- Text Content (Left Side) -->
                    <div style="flex: 2;">
                        <h3>Concept</h3>
                        <p>
                            A Multi-Layer Perceptron (MLP) is a neural network with interconnected layers of neurons,
                            including <strong>hidden layers</strong> between the input and output layers.
                            Each neuron processes inputs through a function, and its output is passed to the next layer,
                            mimicking the hierarchical processing of the human brain.
                        </p>

                        <h3>Key Features</h3>
                        <ul>
                            <li><strong>Function Composition</strong>:
                                <ul>
                                    <li>Neurons perform operations like addition, subtraction, multiplication, or
                                        non-linear functions (e.g., ReLU, sigmoid).</li>
                                    <li>Example: \( \log(4x) \) can be represented as \( f_1(f_2(4, x)) \), where:
                                        <ul>
                                            <li>\( f_2(x) = a \cdot x \) (linear function)</li>
                                            <li>\( f_1(x) = \log(x) \) (non-linear function)</li>
                                        </ul>
                                    </li>
                                </ul>
                            </li>
                            <li><strong>Hierarchical Processing</strong>:
                                <ul>
                                    <li>Input → Hidden Layer 1 → Hidden Layer 2 → ... → Output.</li>
                                    <li>Each layer transforms data to learn increasingly abstract features.</li>
                                </ul>
                            </li>
                        </ul>

                        <h3>Mathematical Formulation</h3>
                        <p>
                            For a neuron in layer \( l \), the output \( a^{(l)} \) is computed as:
                            \[
                            a^{(l)} = \sigma\left( W^{(l)} a^{(l-1)} + b^{(l)} \right)
                            \]
                            Where:
                        <ul>
                            <li>\( \sigma \): Activation function (e.g., sigmoid, ReLU)</li>
                            <li>\( W^{(l)} \): Weight matrix for layer \( l \)</li>
                            <li>\( b^{(l)} \): Bias vector for layer \( l \)</li>
                        </ul>
                        </p>

                        <h3>Applications</h3>
                        <ul>
                            <li>Classification tasks (e.g., image recognition)</li>
                            <li>Regression analysis</li>
                            <li>Feature learning</li>
                        </ul>
                    </div>

                    <!-- Image (Right Side) -->
                    <div style="flex: 1; margin-top: 20px;">
                        <div style="text-align: center;">
                            <img src="../assets/images/7Multilayer-Perceptron.webp" alt="MLP Architecture"
                                style="max-width: 100%; height: auto;">
                            <p><em>Conceptual diagram of an MLP with input, hidden, and output layers.</em></p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Training a Single-Neuron Model</h2>

                <div style="display: flex; gap: 30px; align-items: flex-start;">
                    <!-- Text Content -->
                    <div style="flex: 2;">
                        <h3>1. Model Structure</h3>
                        <p>A single-neuron model consists of:</p>
                        <ul>
                            <li><strong>Inputs</strong> (\(x_1, x_2, \ldots, x_n\)): Features from the data.</li>
                            <li><strong>Weights</strong> (\(w_1, w_2, \ldots, w_n\)): Learnable parameters.</li>
                            <li><strong>Bias</strong> (\(b\)): Shifts the decision boundary.</li>
                            <li><strong>Activation Function</strong> (\(\sigma\)): Converts the weighted sum into an
                                output (e.g., sigmoid, ReLU).</li>
                            <li><strong>Output</strong> (\(\hat{y}\)): Prediction based on inputs and weights.</li>
                        </ul>

                        <h3>2. Forward Propagation</h3>
                        <p>Compute the output \(\hat{y}\):</p>
                        <p>\[
                            z = \sum_{i=1}^n w_i x_i + b \quad \text{(Weighted sum)}
                            \]</p>
                        <p>\[
                            \hat{y} = \sigma(z) \quad \text{(Activation)}
                            \]</p>
                        <p><em>Example:</em> For binary classification, use the <strong>sigmoid</strong> activation:
                            \[
                            \sigma(z) = \frac{1}{1 + e^{-z}}
                            \]</p>

                        <h3>3. Loss Function</h3>
                        <p>Measure prediction error using a loss function:</p>
                        <ul>
                            <li><strong>Binary Cross-Entropy Loss</strong> (classification):
                                \[
                                \mathcal{L} = -\frac{1}{m} \sum_{i=1}^m \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 -
                                \hat{y}_i) \right]
                                \]
                            </li>
                            <li><strong>Mean Squared Error</strong> (regression):
                                \[
                                \mathcal{L} = \frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2
                                \]
                            </li>
                        </ul>

                        <h3>4. Backpropagation</h3>
                        <p>Update weights and bias using gradient descent:</p>
                        <ol>
                            <li>Compute gradients:
                                \[
                                \frac{\partial \mathcal{L}}{\partial w_j} = \frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)
                                x_j^{(i)} \quad \text{(For linear activation)}
                                \]
                            </li>
                            <li>Update parameters:
                                \[
                                w_j \leftarrow w_j - \eta \frac{\partial \mathcal{L}}{\partial w_j}
                                \]
                                \[
                                b \leftarrow b - \eta \frac{\partial \mathcal{L}}{\partial b}
                                \]
                                <em>Where \(\eta\) is the learning rate.</em>
                            </li>
                        </ol>

                        <h3>5. Training Steps</h3>
                        <ol>
                            <li>Initialize weights (\(w\)) and bias (\(b\)) randomly.</li>
                            <li>Iterate for multiple epochs:
                                <ul>
                                    <li>Compute \(\hat{y}\) (forward pass).</li>
                                    <li>Calculate loss \(\mathcal{L}\).</li>
                                    <li>Compute gradients (backward pass).</li>
                                    <li>Update \(w\) and \(b\).</li>
                                </ul>
                            </li>
                            <li>Stop when loss converges or after a fixed number of epochs.</li>
                        </ol>
                    </div>

                    <!-- Image -->
                    <div style="flex: 1; margin-top: 20px;">
                        <div style="text-align: center;">
                            <img src="https://example.com/single-neuron-diagram.png" alt="Single-Neuron Model"
                                style="max-width: 100%; height: auto;">
                            <p><em>Structure of a single-neuron model with inputs, weights, activation, and output.</em>
                            </p>
                        </div>
                    </div>
                </div>

                <h3>6. Key Considerations</h3>
                <ul>
                    <li><strong>Learning Rate (\(\eta\))</strong>: Controls step size during weight updates (too high →
                        divergence; too low → slow training).</li>
                    <li><strong>Activation Choice</strong>: Sigmoid for classification, ReLU/linear for regression.</li>
                    <li><strong>Linearity</strong>: Single neurons can only model linear decision boundaries.</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Training an MLP: Chain Rule for Backpropagation</h2>

                <div style="display: flex; gap: 30px; align-items: flex-start;">
                    <!-- Text Content -->
                    <div style="flex: 2;">
                        <h3>1. Overview</h3>
                        <p>
                            Training a Multi-Layer Perceptron (MLP) involves adjusting weights and biases using the
                            <strong>chain rule</strong>
                            to compute gradients during backpropagation. This allows the network to minimize the loss
                            function by propagating
                            errors backward from the output layer to the input layer.
                        </p>

                        <h3>2. Mathematical Framework</h3>
                        <p>Consider an MLP with:</p>
                        <ul>
                            <li><strong>Input</strong>: \( x \in \mathbb{R}^d \)</li>
                            <li><strong>Hidden Layer</strong>: \( h = \sigma(W_1 x + b_1) \), where \( \sigma \) is the
                                activation function (e.g., ReLU).</li>
                            <li><strong>Output</strong>: \( \hat{y} = \sigma(W_2 h + b_2) \)</li>
                            <li><strong>Loss</strong>: \( \mathcal{L} = \frac{1}{2}(y - \hat{y})^2 \) (Mean Squared
                                Error).</li>
                        </ul>

                        <h3>3. Chain Rule in Backpropagation</h3>
                        <p>To update weights (\( W_1, W_2 \)) and biases (\( b_1, b_2 \)), compute gradients using the
                            chain rule:</p>
                        <ol>
                            <li><strong>Output Layer Gradients</strong>:
                                \[
                                \frac{\partial \mathcal{L}}{\partial W_2} = \frac{\partial \mathcal{L}}{\partial
                                \hat{y}} \cdot \frac{\partial \hat{y}}{\partial W_2} = (y - \hat{y}) \cdot \sigma'(W_2 h
                                + b_2) \cdot h^T
                                \]
                            </li>
                            <li><strong>Hidden Layer Gradients</strong>:
                                \[
                                \frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial h}
                                \cdot \frac{\partial h}{\partial W_1} = \left[ W_2^T \cdot (y - \hat{y}) \cdot
                                \sigma'(W_2 h + b_2) \right] \cdot \sigma'(W_1 x + b_1) \cdot x^T
                                \]
                            </li>
                        </ol>

                        <h3>4. Step-by-Step Gradient Calculation</h3>
                        <p>For a network with ReLU activation (\( \sigma(z) = \max(0, z) \)):</p>
                        <ul>
                            <li><strong>Forward Pass</strong>:
                                <ol>
                                    <li>Compute hidden layer: \( h = \text{ReLU}(W_1 x + b_1) \)</li>
                                    <li>Compute output: \( \hat{y} = \text{ReLU}(W_2 h + b_2) \)</li>
                                </ol>
                            </li>
                            <li><strong>Backward Pass</strong>:
                                <ol>
                                    <li>Compute output layer error: \( \delta_{\text{out}} = (y - \hat{y}) \cdot
                                        \text{ReLU}'(W_2 h + b_2) \)</li>
                                    <li>Compute hidden layer error: \( \delta_{\text{hidden}} = W_2^T
                                        \delta_{\text{out}} \cdot \text{ReLU}'(W_1 x + b_1) \)</li>
                                    <li>Update weights:
                                        \[
                                        W_2 \leftarrow W_2 + \eta \cdot \delta_{\text{out}} \cdot h^T, \quad
                                        W_1 \leftarrow W_1 + \eta \cdot \delta_{\text{hidden}} \cdot x^T
                                        \]
                                    </li>
                                </ol>
                            </li>
                        </ul>
                    </div>

                    <!-- Image Placeholder -->
                    <div style="flex: 1; margin-top: 20px;">
                        <div style="text-align: center;">
                            <img src="https://example.com/mlp-chain-rule.png" alt="MLP Chain Rule Diagram"
                                style="max-width: 100%; height: auto;">
                            <p><em>Backpropagation uses the chain rule to compute gradients across layers.</em></p>
                        </div>
                    </div>
                </div>

                <h3>5. Key Equations</h3>
                <table border="1">
                    <tr>
                        <th>Gradient Type</th>
                        <th>Formula</th>
                    </tr>
                    <tr>
                        <td><strong>Output Layer Weights</strong></td>
                        <td>\( \frac{\partial \mathcal{L}}{\partial W_2} = \delta_{\text{out}} \cdot h^T \)</td>
                    </tr>
                    <tr>
                        <td><strong>Hidden Layer Weights</strong></td>
                        <td>\( \frac{\partial \mathcal{L}}{\partial W_1} = \delta_{\text{hidden}} \cdot x^T \)</td>
                    </tr>
                    <tr>
                        <td><strong>Output Layer Bias</strong></td>
                        <td>\( \frac{\partial \mathcal{L}}{\partial b_2} = \delta_{\text{out}} \)</td>
                    </tr>
                    <tr>
                        <td><strong>Hidden Layer Bias</strong></td>
                        <td>\( \frac{\partial \mathcal{L}}{\partial b_1} = \delta_{\text{hidden}} \)</td>
                    </tr>
                </table>

                <h3>6. Why the Chain Rule Matters</h3>
                <ul>
                    <li>Efficiently computes gradients for deep networks by breaking derivatives into local components.
                    </li>
                    <li>Enables optimization algorithms like SGD to update weights iteratively.</li>
                </ul>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Memoization in MLP Training</h2>

                <div style="display: flex; gap: 30px; align-items: flex-start;">
                    <!-- Text Content -->
                    <div style="flex: 2;">
                        <h3>1. What is Memoization?</h3>
                        <p>
                            Memoization is an optimization technique that stores the results of computationally
                            expensive operations during the <strong>forward pass</strong>
                            (e.g., layer activations, intermediate derivatives) to reuse them in the <strong>backward
                                pass</strong>. This avoids redundant recalculations,
                            significantly speeding up training.
                        </p>

                        <h3>2. Memoization in MLPs</h3>
                        <p>During training, an MLP performs:</p>
                        <ul>
                            <li><strong>Forward Pass</strong>:
                                <ul>
                                    <li>Computes layer outputs (e.g., \( z_1 = W_1 x + b_1 \), \( a_1 = \sigma(z_1) \)).
                                    </li>
                                    <li>Stores these values in memory (memoization).</li>
                                </ul>
                            </li>
                            <li><strong>Backward Pass</strong>:
                                <ul>
                                    <li>Uses cached values to compute gradients via the chain rule.</li>
                                    <li>Example: Reuses \( z_1 \) and \( a_1 \) to calculate \( \frac{\partial
                                        \mathcal{L}}{\partial W_1} \).</li>
                                </ul>
                            </li>
                        </ul>



                        <h3>3. Code Example: Memoization in PyTorch</h3>
                        <pre style="background: #373434; padding: 10px; border-radius: 5px;">
                            <code>
            import torch
            
            # Forward pass (memoizes z1, a1, z2)
            x = torch.randn(10, 5)  # Input
            z1 = torch.matmul(x, W1) + b1  # Memoized
            a1 = torch.relu(z1)            # Memoized
            z2 = torch.matmul(a1, W2) + b2 # Memoized
            
            # Backward pass (uses cached values)
            loss = ((z2 - y)**2).mean()
            loss.backward()  # Gradients reuse z1, a1, z2
                            </code>
                        </pre>

                        <h3>4. Trade-offs</h3>
                        <ul>
                            <li><strong>Speed</strong>: Reduces computation time by ~50%.</li>
                            <li><strong>Memory</strong>: Requires storing intermediate values (challenge for large
                                models).</li>
                        </ul>
                    </div>


                </div>

                <h3>5. Why It Matters</h3>
                <p>
                    Memoization is foundational for efficient deep learning. Frameworks like PyTorch and TensorFlow
                    automate this process through
                    <strong>computation graphs</strong>, enabling scalable training of complex models like GPT-3 and
                    ResNet.
                </p>
            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2><em>Back Propagation</em></h2>
                <h3>Backpropagation</h3>
                <ul>
                    <li>Backpropagation is a combination of memoization & chain rule. The procedure is as follows:
                        <ul>
                            <li>Initialize \( w_{ij}^{k} \)</li>
                            <li>For each \( x_i \) in \( D \)
                                <ul>
                                    <li>Forward Propagation: Pass \( x_i \) forward through the network</li>
                                    <li>Compute Loss function \( L(y_i, \hat{y_i}) \)</li>
                                    <li>Compute all derivatives using chain rule & memoization</li>
                                    <li>Update the weights again from end to the start such that loss will be minimized
                                    </li>
                                </ul>
                            </li>
                            <li>Repeat step 2 till convergence</li>
                        </ul>
                    </li>
                    <li><strong>Note:</strong> Backpropagation works only if activation functions are differentiable. If
                        these functions are easily or fast differentiable, it speeds up the training & Neural Network.
                    </li>
                </ul>
            </div>
        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div class="failure-cases">
                <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                    <div>
                        <h2>Activation Functions</h2>

                        <div style="display: flex; gap: 20px; align-items: flex-start;">
                            <!-- Left Column - Text Content -->
                            <div style="flex: 3;">
                                <h3>Sigmoid (σ)</h3>

                                <div style="margin-left: 20px;">
                                    <h4>1. Overview</h4>
                                    <p>
                                        The sigmoid function is a non-linear activation function that squashes input
                                        values
                                        between 0 and 1. It is particularly useful for binary classification tasks.
                                    </p>

                                    <h4>2. Mathematical Definition</h4>
                                    <p>The sigmoid function is defined as:</p>
                                    <div style="margin: 15px 0;">
                                        <p>
                                            \[
                                            \sigma(z) = \frac{1}{1 + e^{-z}}
                                            \]
                                            <em>Where \( z = W \cdot x + b \) (weighted sum of inputs + bias).</em>
                                        </p>
                                    </div>

                                    <h4>3. Key Properties</h4>
                                    <ul>
                                        <li>Output range: \( (0, 1) \)</li>
                                        <li>Derivative range: \( (0, 0.25) \)</li>
                                        <li>Smooth, S-shaped curve</li>
                                    </ul>

                                    <h4>4. Derivation of the Derivative</h4>
                                    <p>The derivative of the sigmoid function is critical for backpropagation:</p>
                                    <ol>
                                        <li>Start with \( \sigma(z) = \frac{1}{1 + e^{-z}} \)</li>
                                        <li>Let \( g(z) = 1 + e^{-z} \), so \( \sigma(z) = g(z)^{-1} \)</li>
                                        <li>Apply the chain rule:
                                            \[
                                            \frac{d\sigma}{dz} = \frac{d\sigma}{dg} \cdot \frac{dg}{dz}
                                            \]
                                        </li>
                                        <li>Calculate individual terms:
                                            \[
                                            \frac{d\sigma}{dg} = -g^{-2}, \quad \frac{dg}{dz} = -e^{-z}
                                            \]
                                        </li>
                                        <li>Combine results:
                                            \[
                                            \frac{d\sigma}{dz} = \frac{e^{-z}}{(1 + e^{-z})^2} = \sigma(z)(1 -
                                            \sigma(z))
                                            \]
                                        </li>
                                    </ol>

                                    <h4>6. Limitations</h4>
                                    <ul>
                                        <li>Vanishing gradients for extreme inputs (saturates at 0/1)</li>
                                        <li>Not zero-centered (can slow down learning)</li>
                                    </ul>
                                </div>
                            </div>

                            <!-- Right Column - Image -->
                            <div style="flex: 1; margin-top: 20px;">
                                <div style="text-align: center;">
                                    <img src="../assets/images/sigmoid.jpg" alt="Sigmoid Function Graph"
                                        style="max-width: 80%; height: auto; border: 1px solid #ddd; padding: 10px;">
                                    <p><em>Sigmoid function and its derivative (dashed line).</em></p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="failure-cases">
                <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                    <div>
                        <h2>Activation Functions</h2>

                        <!-- Tanh Section -->
                        <div style="display: flex; gap: 30px; align-items: flex-start;">
                            <!-- Left Column - Text Content -->
                            <div style="flex: 3;">
                                <h3>Hyperbolic Tangent (tanh)</h3>
                                <div style="margin-left: 20px;">
                                    <h4>1. Overview</h4>
                                    <p>
                                        The tanh function is a zero-centered activation function that squashes input
                                        values
                                        between -1 and 1. It is preferred over sigmoid for hidden layers in many
                                        architectures due to its
                                        symmetry.
                                    </p>

                                    <h4>2. Mathematical Definition</h4>
                                    <div style="margin: 15px 0;">
                                        <p>
                                            \[
                                            \tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
                                            \]
                                        </p>
                                    </div>

                                    <h4>3. Key Properties</h4>
                                    <ul>
                                        <li>Output range: \( (-1, 1) \)</li>
                                        <li>Derivative range: \( (0, 1) \)</li>
                                        <li>Zero-centered (mitigates learning slowdown)</li>
                                    </ul>

                                    <h4>4. Derivation of the Derivative</h4>
                                    <ol>
                                        <li>Start with \( \tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \)</li>
                                        <li>Differentiate using quotient rule:
                                            \[
                                            \frac{d}{dz} \tanh(z) = 1 - \tanh^2(z)
                                            \]
                                        </li>
                                    </ol>

                                    <h4>6. Limitations</h4>
                                    <ul>
                                        <li>Still suffers from vanishing gradients for extreme values</li>
                                        <li>Computationally more expensive than ReLU</li>
                                    </ul>
                                </div>
                            </div>

                            <!-- Right Column - Image -->
                            <div style="flex: 1; margin-top: 10px;">
                                <div style="text-align: center;">
                                    <img src="../assets/images/tanh.png" alt="tanh Graph"
                                        style="max-width: 80%; height: auto; border: 1px solid #ddd; padding: 10px;">
                                    <p><em>tanh function (solid) and its derivative (dashed).</em></p>
                                </div>
                            </div>
                        </div>

                        <!-- ReLU Section -->
                    </div>
                </div>
            </div>

            <div class="failure-cases">
                <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
                    <div>
                        <h2>Activation Functions</h2>

                        <!-- ReLU Section -->
                        <div style="display: flex; gap: 30px; align-items: flex-start;">
                            <!-- Left Column - Text Content -->
                            <div style="flex: 3;">
                                <h3>Rectified Linear Unit (ReLU)</h3>
                                <div style="margin-left: 20px;">
                                    <h4>1. Overview</h4>
                                    <p>
                                        ReLU is the most widely used activation function for deep networks. It outputs
                                        the input
                                        directly if positive; otherwise, it outputs zero. Computationally efficient and
                                        mitigates vanishing gradients.
                                    </p>

                                    <h4>2. Mathematical Definition</h4>
                                    <div style="margin: 15px 0;">
                                        <p>
                                            \[
                                            \text{ReLU}(z) = \max(0, z)
                                            \]
                                        </p>
                                    </div>

                                    <h4>3. Key Properties</h4>
                                    <ul>
                                        <li>Output range: \( [0, \infty) \)</li>
                                        <li>Derivative: 1 (if \( z > 0 \)), 0 (otherwise)</li>
                                        <li>Sparsity-inducing (reduces overfitting)</li>
                                    </ul>

                                    <h4>4. Derivation of the Derivative</h4>
                                    <ol>
                                        <li>For \( z > 0 \): \( \frac{d}{dz} \text{ReLU}(z) = 1 \)</li>
                                        <li>For \( z \leq 0 \): \( \frac{d}{dz} \text{ReLU}(z) = 0 \)</li>
                                    </ol>

                                    <h4>6. Limitations</h4>
                                    <ul>
                                        <li>"Dying ReLU" problem: Neurons can get stuck in negative region</li>
                                        <li>Non-differentiable at \( z = 0 \)</li>
                                    </ul>
                                </div>
                            </div>

                            <!-- Right Column - Image -->
                            <div style="flex: 1; margin-top: 20px;">
                                <div style="text-align: center;">
                                    <img src="../assets/images/relu.jpg" alt="ReLU Graph"
                                        style="max-width: 80%; height: auto; border: 1px solid #ddd; padding: 10px;">
                                    <p><em>ReLU function (solid) and its derivative (dashed).</em></p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>The Vanishing/Exploding Gradient Problem</h2>

                <div style="margin: 20px 0; border: 1px solid #eee; padding: 15px; border-radius: 8px;">
                    <h3>1. Overview</h3>
                    <p>
                        In deep neural networks, gradients computed during backpropagation involve multiplying
                        derivatives across layers.
                        This can cause:
                    </p>
                    <ul>
                        <li><strong>Exploding Gradients</strong>: Gradients grow exponentially if derivatives > 1.</li>
                        <li><strong>Vanishing Gradients</strong>: Gradients shrink to near-zero if derivatives < 1.</li>
                    </ul>
                </div>

                <div style="margin: 20px 0; border: 1px solid #eee; padding: 15px; border-radius: 8px;">
                    <h3>2. Mathematical Explanation</h3>
                    <p>For a network with \( n \) layers, the gradient at the first layer is:</p>
                    <p>\[
                        \frac{\partial \mathcal{L}}{\partial W_1} = \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot
                        \prod_{k=2}^n \frac{\partial h_k}{\partial h_{k-1}}
                        \]</p>
                    <ul>
                        <li>If \( \frac{\partial h_k}{\partial h_{k-1}} > 1 \): Gradient <strong>explodes</strong>
                            (e.g., \( 1.5^{10} \approx 58 \)).</li>
                        <li>If \( \frac{\partial h_k}{\partial h_{k-1}} < 1 \): Gradient <strong>vanishes</strong>
                                (e.g., \( 0.5^{10} \approx 0.001 \)).</li>
                    </ul>
                </div>

                <div style="margin: 20px 0; border: 1px solid #eee; padding: 15px; border-radius: 8px;">
                    <h3>3. Causes</h3>
                    <h4>Vanishing Gradients</h4>
                    <ul>
                        <li>Activation functions with small derivatives (e.g., sigmoid, tanh).</li>
                        <li>Deep networks amplify the multiplicative effect.</li>
                    </ul>
                    <h4>Exploding Gradients</h4>
                    <ul>
                        <li>Large initial weights or improper weight initialization.</li>
                        <li>Unbounded activations (e.g., ReLU without normalization).</li>
                    </ul>
                </div>

                <div style="margin: 20px 0; border: 1px solid #eee; padding: 15px; border-radius: 8px;">
                    <h3>4. Solutions</h3>
                    <h4>For Vanishing Gradients</h4>
                    <ul>
                        <li>Use ReLU/Leaky ReLU activations.</li>
                        <li>Weight initialization (He/Xavier).</li>
                        <li>Skip connections (ResNet).</li>
                    </ul>
                    <h4>For Exploding Gradients</h4>
                    <ul>
                        <li>Gradient clipping (<code>torch.nn.utils.clip_grad_norm_</code>).</li>
                        <li>Batch normalization.</li>
                        <li>L2 regularization.</li>
                    </ul>
                </div>

                <div style="margin: 20px 0; border: 1px solid #eee; padding: 15px; border-radius: 8px;">
                    <h3>5. Activation Function Comparison</h3>
                    <table style="width: 100%; border-collapse: collapse;">
                        <tr style="background-color: #f5f5f5;">
                            <th style="padding: 10px; border: 1px solid #ddd;">Activation</th>
                            <th style="padding: 10px; border: 1px solid #ddd;">Derivative Range</th>
                            <th style="padding: 10px; border: 1px solid #ddd;">Gradient Behavior</th>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">Sigmoid</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">\( (0, 0.25] \)</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Vanishes quickly</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">Tanh</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">\( (0, 1] \)</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Vanishes slower</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">ReLU</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">\( \{0, 1\} \)</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Mitigates vanishing</td>
                        </tr>
                    </table>
                </div>

                <div style="margin: 20px 0; border: 1px solid #eee; padding: 15px; border-radius: 8px;">
                    <h3>6. Conclusion</h3>
                    <p>
                        The vanishing/exploding gradient problem is critical in deep learning but can be managed with
                        techniques like
                        ReLU, careful initialization, gradient clipping, and architectural innovations (e.g., ResNet).
                        These strategies
                        enable stable training of very deep networks.
                    </p>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Key Techniques for Improving Neural Network Training</h2>

                <!-- Dropout Section -->
                <div style="margin: 20px 0; padding: 15px; border: 1px solid #eee; border-radius: 8px;">
                    <h3>1. Dropout</h3>
                    <p><strong>Purpose:</strong> Prevent overfitting by introducing stochasticity during training.</p>
                    <p><strong>How It Works:</strong></p>
                    <ul>
                        <li>Randomly "drops" (deactivates) neurons during each training iteration (<em>dropout rate</em>
                            = probability of dropping a neuron).</li>
                        <li>Example: With a dropout rate of 0.5, 50% of neurons are temporarily ignored in each forward
                            pass.</li>
                        <li>During inference, all neurons are active, but their outputs are scaled by the dropout rate
                            to maintain expected values.</li>
                    </ul>
                    <p><strong>Benefits:</strong></p>
                    <ul>
                        <li>Reduces co-adaptation of neurons (forces redundancy)</li>
                        <li>Acts as a form of <em>stochastic regularization</em></li>
                    </ul>
                </div>

                <!-- Regularization Section -->
                <div style="margin: 20px 0; padding: 15px; border: 1px solid #eee; border-radius: 8px;">
                    <h3>2. Regularization</h3>
                    <p><strong>Purpose:</strong> Prevent overfitting by discouraging overly complex models.</p>
                    <p><strong>Common Types:</strong></p>
                    <table style="width: 100%; border-collapse: collapse; margin: 10px 0;">
                        <tr style="background-color: #f5f5f5;">
                            <th style="padding: 10px; border: 1px solid #ddd;">Type</th>
                            <th style="padding: 10px; border: 1px solid #ddd;">Mechanism</th>
                            <th style="padding: 10px; border: 1px solid #ddd;">Effect</th>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">L1 (Lasso)</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Adds \( \lambda \sum |w| \) to loss</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Sparse weights</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">L2 (Ridge)</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Adds \( \lambda \sum w^2 \) to loss</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Small weights</td>
                        </tr>
                    </table>
                    <p><strong>Key Difference:</strong> L1 creates sparse models, while L2 distributes error across
                        weights.</p>
                </div>

                <!-- Batch Normalization Section -->
                <div style="margin: 20px 0; padding: 15px; border: 1px solid #eee; border-radius: 8px;">
                    <h3>3. Batch Normalization</h3>
                    <p><strong>Purpose:</strong> Stabilize and accelerate training by reducing internal covariate shift.
                    </p>
                    <p><strong>How It Works:</strong></p>
                    <ol>
                        <li>Normalizes layer inputs to \( \mathcal{N}(0, 1) \) using batch statistics:
                            \[
                            \hat{x} = \frac{x - \mu_{\text{batch}}}{\sqrt{\sigma_{\text{batch}}^2 + \epsilon}}
                            \]
                        </li>
                        <li>Applies learnable scale (\(\gamma\)) and shift (\(\beta\)) parameters:
                            \[
                            y = \gamma \hat{x} + \beta
                            \]
                        </li>
                    </ol>
                    <p><strong>Benefits:</strong></p>
                    <ul>
                        <li>Allows higher learning rates</li>
                        <li>Reduces sensitivity to weight initialization</li>
                        <li>Mild regularization effect due to noise in batch statistics</li>
                    </ul>
                </div>

                <!-- Comparison Section -->
                <div style="margin: 20px 0; padding: 15px; border: 1px solid #eee; border-radius: 8px;">
                    <h3>4. Key Differences & Interactions</h3>
                    <table style="width: 100%; border-collapse: collapse;">
                        <tr style="background-color: #f5f5f5;">
                            <th style="padding: 10px; border: 1px solid #ddd;">Technique</th>
                            <th style="padding: 10px; border: 1px solid #ddd;">Primary Role</th>
                            <th style="padding: 10px; border: 1px solid #ddd;">When to Use</th>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">Dropout</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Regularization</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Large networks prone to overfitting</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">L1/L2</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Regularization</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Feature selection (L1) or general
                                overfitting (L2)</td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; border: 1px solid #ddd;">BatchNorm</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Training Stabilization</td>
                            <td style="padding: 10px; border: 1px solid #ddd;">Deep networks with unstable gradients
                            </td>
                        </tr>
                    </table>
                    <p><strong>Note:</strong> BatchNorm may reduce the need for dropout in some cases - often used
                        together with L2 regularization.</p>
                </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Weight Initialization Techniques in Deep Neural Networks (DNNs)</h2>

                <h3>1. Zero Initialization</h3>
                <p><strong>Formula:</strong> W = 0</p>
                <p>All weights are set to zero, causing neurons to learn the same features, preventing learning.</p>

                <h3>2. Random Initialization</h3>
                <p><strong>Formula:</strong> W ∼ U(-ε, ε)</p>
                <p>Weights are randomly sampled from a small range but do not consider layer size.</p>

                <h3>3. Xavier (Glorot) Initialization</h3>
                <p><strong>Formula (Uniform Distribution):</strong></p>
                <p>W ∼ U(-√(6/(n_in + n_out)), √(6/(n_in + n_out)))</p>
                <p><strong>Formula (Normal Distribution):</strong></p>
                <p>W ∼ 𝒩(0, 1/(n_in + n_out))</p>
                <p>Balances variance to prevent vanishing/exploding gradients. Best for sigmoid and tanh activations.
                </p>

                <h3>4. He (Kaiming) Initialization</h3>
                <p><strong>Formula (Uniform Distribution):</strong></p>
                <p>W ∼ U(-√(6/n_in), √(6/n_in))</p>
                <p><strong>Formula (Normal Distribution):</strong></p>
                <p>W ∼ 𝒩(0, 2/n_in)</p>
                <p>Best suited for ReLU and Leaky ReLU activations, preventing dead neurons.</p>

                <h3>5. Lecun Initialization</h3>
                <p><strong>Formula (Uniform Distribution):</strong></p>
                <p>W ∼ U(-√(3/n_in), √(3/n_in))</p>
                <p><strong>Formula (Normal Distribution):</strong></p>
                <p>W ∼ 𝒩(0, 1/n_in)</p>
                <p>Optimized for tanh and SELU activations.</p>

                <h3>6. Orthogonal Initialization</h3>
                <p><strong>Formula:</strong> W = Q / ||Q||</p>
                <p>Uses QR decomposition to stabilize gradient flow, mainly for RNNs.</p>

                <h3>7. Variance Scaling Initialization</h3>
                <p><strong>Formula (ReLU):</strong> W ∼ 𝒩(0, 2/n_in)</p>
                <p><strong>Formula (SELU):</strong> W ∼ 𝒩(0, 1/n_in)</p>
                <p>Dynamically adjusts variance based on activation functions.</p>

                <h3>Choosing the Right Initialization</h3>
                <table border="1">
                    <tr>
                        <th>Activation Function</th>
                        <th>Recommended Initialization</th>
                    </tr>
                    <tr>
                        <td>Sigmoid, Tanh</td>
                        <td>Xavier (Glorot)</td>
                    </tr>
                    <tr>
                        <td>ReLU, Leaky ReLU</td>
                        <td>He (Kaiming)</td>
                    </tr>
                    <tr>
                        <td>SELU</td>
                        <td>Lecun</td>
                    </tr>
                    <tr>
                        <td>RNNs</td>
                        <td>Orthogonal</td>
                    </tr>
                </table>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Batch Normalization in Deep Learning</h2>

                <h3>What is Batch Normalization?</h3>
                <p>Batch Normalization (BatchNorm) is a technique used in deep neural networks to normalize activations
                    of each layer during training. It improves training stability, prevents vanishing/exploding
                    gradients, and speeds up convergence.</p>

                <h3>Why is Batch Normalization Needed?</h3>
                <ul>
                    <li><strong>Internal Covariate Shift:</strong> Stabilizes shifting activation distributions.</li>
                    <li><strong>Faster Convergence:</strong> Reduces training time by keeping activations consistent.
                    </li>
                    <li><strong>Prevents Vanishing/Exploding Gradients:</strong> Keeps values in a stable range.</li>
                    <li><strong>Regularization Effect:</strong> Acts as a mild form of regularization, reducing reliance
                        on dropout.</li>
                </ul>

                <h3>Batch Normalization Formula</h3>
                <p><strong>1. Compute Mean & Variance for Mini-Batch:</strong></p>
                <p>&nbsp;&nbsp;&nbsp;&nbsp;μ_B = (1/m) Σ x_i</p>
                <p>&nbsp;&nbsp;&nbsp;&nbsp;σ_B² = (1/m) Σ (x_i - μ_B)²</p>

                <p><strong>2. Normalize the Activations:</strong></p>
                <p>&nbsp;&nbsp;&nbsp;&nbsp;ẋ_i = (x_i - μ_B) / √(σ_B² + ε)</p>

                <p><strong>3. Scale and Shift with Learnable Parameters:</strong></p>
                <p>&nbsp;&nbsp;&nbsp;&nbsp;y_i = γẋ_i + β</p>

                <h3>Where is Batch Normalization Applied?</h3>
                <ul>
                    <li>Before activation functions (common in CNNs)</li>
                    <li>After activation functions (common in fully connected networks)</li>
                    <li>Between layers in deep networks</li>
                </ul>

                <h3>Advantages of Batch Normalization</h3>
                <ul>
                    <li>✅ Reduces sensitivity to weight initialization</li>
                    <li>✅ Speeds up training</li>
                    <li>✅ Improves generalization</li>
                    <li>✅ Prevents internal covariate shift</li>
                    <li>✅ Acts as regularization</li>
                </ul>

                <h3>Batch Normalization in Python</h3>
                <p><strong>In TensorFlow/Keras:</strong></p>
                <pre>
                    <code>
            from tensorflow.keras.layers import BatchNormalization, Dense
            
            model.add(Dense(64, activation='relu'))
            model.add(BatchNormalization())  # Applying BatchNorm
                    </code>
                </pre>

                <p><strong>In PyTorch:</strong></p>
                <pre>
                    <code>
            import torch.nn as nn
            
            nn.BatchNorm1d(num_features=64)  # For Fully Connected Layers
            nn.BatchNorm2d(num_features=64)  # For CNNs
                    </code>
                </pre>
            </div>

        </div>
    </div>

    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Optimizers in Deep Neural Networks (DNNs)</h2>

                <p>Optimizers are algorithms that adjust the weights of a neural network to minimize the loss function.
                    The choice of optimizer affects convergence speed, accuracy, and overall model performance.</p>

                <h3>1. Gradient Descent (GD)</h3>
                <p><strong>Formula:</strong></p>
                <p>W = W - η ∇L(W)</p>
                <p><strong>Why It Was Introduced:</strong> It is the fundamental optimization algorithm used in machine
                    learning, forming the basis for all other optimizers.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Simple and easy to understand.</li>
                    <li>Guaranteed to find a minimum if convex loss function.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Very slow for large datasets (Batch GD).</li>
                    <li>May get stuck in local minima.</li>
                </ul>

                <h3>2. Stochastic Gradient Descent (SGD)</h3>
                <p><strong>Formula:</strong></p>
                <p>W = W - η ∇L(W) (for each sample)</p>
                <p><strong>Why It Was Introduced:</strong> To speed up gradient descent by updating weights after each
                    sample instead of the entire dataset.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Faster than batch GD.</li>
                    <li>Can escape local minima due to noise.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>High variance in updates leads to instability.</li>
                    <li>May not converge to the global minimum.</li>
                </ul>

                <h3>3. Momentum Optimizer</h3>
                <p><strong>Formula:</strong></p>
                <p>v_t = β v_(t-1) + η ∇L(W)</p>
                <p>W = W - v_t</p>
                <p><strong>Why It Was Introduced:</strong> To reduce the oscillations in SGD and speed up convergence.
                </p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Reduces oscillations in deep valleys.</li>
                    <li>Faster convergence in certain cases.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Can overshoot the optimal point.</li>
                </ul>

                <h3>4. Nesterov Accelerated Gradient (NAG)</h3>
                <p><strong>Formula:</strong></p>
                <p>v_t = β v_(t-1) + η ∇L(W - β v_(t-1))</p>
                <p>W = W - v_t</p>
                <p><strong>Why It Was Introduced:</strong> To improve upon momentum by looking ahead before applying
                    updates.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>More accurate updates than momentum.</li>
                    <li>Less overshooting.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Slightly more computationally expensive.</li>
                </ul>

                <h3>5. Adagrad (Adaptive Gradient Algorithm)</h3>
                <p><strong>Formula:</strong></p>
                <p>W = W - (η / √(G_t + ε)) ∇L(W)</p>
                <p><strong>Why It Was Introduced:</strong> To adapt learning rates for each parameter, making it useful
                    for sparse data.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Good for sparse features (e.g., NLP).</li>
                    <li>Adapts learning rate for each weight.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Learning rate decreases too much over time.</li>
                </ul>

                <h3>6. RMSprop (Root Mean Square Propagation)</h3>
                <p><strong>Formula:</strong></p>
                <p>G_t = β G_(t-1) + (1 - β) (∇L(W))²</p>
                <p>W = W - (η / √(G_t + ε)) ∇L(W)</p>
                <p><strong>Why It Was Introduced:</strong> To solve Adagrad’s problem by using an exponentially weighted
                    moving average of past squared gradients.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Prevents drastic learning rate reduction.</li>
                    <li>Works well for non-stationary problems (e.g., RNNs).</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Requires careful tuning of hyperparameters.</li>
                </ul>

                <h3>7. Adam (Adaptive Moment Estimation)</h3>
                <p><strong>Formula:</strong></p>
                <p>m_t = β₁ m_(t-1) + (1 - β₁) ∇L(W)</p>
                <p>v_t = β₂ v_(t-1) + (1 - β₂) (∇L(W))²</p>
                <p>W = W - (η / (√v_t + ε)) m_t</p>
                <p><strong>Why It Was Introduced:</strong> To combine the benefits of Momentum and RMSprop.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Works well for most deep learning models.</li>
                    <li>Combines benefits of Momentum and RMSprop.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Can sometimes lead to non-converging solutions.</li>
                </ul>

                <h3>8. AdamW (Adam with Weight Decay)</h3>
                <p><strong>Formula:</strong></p>
                <p>W = W - (η / (√v_t + ε)) m_t - λW</p>
                <p><strong>Why It Was Introduced:</strong> To improve Adam’s generalization by adding weight decay.</p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Better generalization than Adam.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>Still requires tuning of hyperparameters.</li>
                </ul>

                <h3>9. Nadam (Nesterov-Accelerated Adam)</h3>
                <p><strong>Formula:</strong></p>
                <p>W = W - (η / (√v_t + ε)) (m_t + β₁ m_(t-1))</p>
                <p><strong>Why It Was Introduced:</strong> To further improve Adam by incorporating Nesterov momentum.
                </p>

                <h4>✅ Pros:</h4>
                <ul>
                    <li>Faster convergence than Adam.</li>
                </ul>
                <h4>❌ Cons:</h4>
                <ul>
                    <li>More computationally expensive.</li>
                </ul>

                <h3>Conclusion</h3>
                <p>Each optimizer was introduced to overcome a limitation of previous ones. The choice depends on the
                    problem: SGD for simplicity, Momentum/NAG for faster convergence, Adagrad/RMSprop for adaptive
                    learning rates, and Adam/AdamW/Nadam for general deep learning tasks.</p>
            </div>

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h2>Gradient Clipping in Deep Neural Networks</h2>

                <p>Gradient Clipping is a technique used to prevent the problem of exploding gradients, which can occur
                    in deep networks, particularly in recurrent neural networks (RNNs) and long training processes.</p>

                <h3>Why Is Gradient Clipping Needed?</h3>
                <p>During backpropagation, gradients can become excessively large, leading to unstable updates in weight
                    parameters. This can cause:</p>
                <ul>
                    <li>Loss function divergence (failure to converge).</li>
                    <li>Numerical instability (NaN values in computations).</li>
                    <li>Poor generalization due to chaotic weight updates.</li>
                </ul>

                <h3>Types of Gradient Clipping</h3>

                <h4>1. Norm-Based Gradient Clipping</h4>
                <p>This method ensures that the gradient norm does not exceed a predefined threshold.</p>

                <p><strong>Formula:</strong></p>
                <p>If ||g|| ≥ c, then scale the gradient as:</ </div>
            </div>
        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">

        </div>
    </div>
    <div class="failure-cases">
        <div class="case" style="font-family: 'Courier New', monospace; padding: 10px; border: 1px solid #ddd;">
            <div>
                <h1>Memoization: Optimizing Functions with Caching</h1>

                <h2>What is Memoization?</h2>
                <p>
                    Memoization is an optimization technique that speeds up programs by <strong>caching the
                        results</strong> of expensive function calls.
                    When the function is called again with the same arguments, it returns the precomputed result
                    instead
                    of recalculating it.
                </p>

                <h2>How Memoization Works</h2>
                <ol>
                    <li><strong>Check Cache</strong>: Before executing the function, check if the arguments
                        exist in
                        a
                        cache (e.g., a hash table).</li>
                    <li><strong>Return Cached Result</strong>: If the arguments are found, return the stored
                        result.
                    </li>
                    <li><strong>Compute & Cache</strong>: If not, compute the result, store it in the cache, and
                        return
                        it.</li>
                </ol>

                <h2>Example: Fibonacci Sequence</h2>
                <div style="display: flex; gap: 20px;">
                    <!-- Without Memoization -->
                    <div style="flex: 1; border: 1px solid #ccc; padding: 10px;">
                        <h3>Inefficient Recursive Fibonacci</h3>
                        <pre>
        def fib(n):
            if n <= 1:
                return n
            return fib(n-1) + fib(n-2)
        
        print(fib(35))  # Slow due to redundant calculations
                    </pre>
                    </div>

                    <!-- With Memoization -->
                    <div style="flex: 1; border: 1px solid #ccc; padding: 10px;">
                        <h3>Memoized Fibonacci (Python)</h3>
                        <pre>
        from functools import lru_cache
        
        @lru_cache(maxsize=None)
        def fib_memo(n):
            if n <= 1:
                return n
            return fib_memo(n-1) + fib_memo(n-2)
        
        print(fib_memo(35))  # Fast due to cached results
                    </pre>
                    </div>
                </div>

                <h2>Use Cases</h2>
                <ul>
                    <li><strong>Recursive Algorithms</strong>: Avoid redundant calls (e.g., Fibonacci,
                        factorial).
                    </li>
                    <li><strong>Dynamic Programming</strong>: Solve overlapping subproblems efficiently (e.g.,
                        knapsack,
                        shortest path).</li>
                    <li><strong>API Calls</strong>: Cache responses for repeated requests with the same
                        parameters.
                    </li>
                </ul>

                <h2>Pros and Cons</h2>
                <table border="1" style="width: 100%; margin: 10px 0;">
                    <tr>
                        <th>Pros</th>
                        <th>Cons</th>
                    </tr>
                    <tr>
                        <td>Reduces time complexity (e.g., Fibonacci from O(2ⁿ) to O(n))</td>
                        <td>Increases memory usage (stores cached results)</td>
                    </tr>
                    <tr>
                        <td>Improves performance for repeated calls</td>
                        <td>Not suitable for functions with side effects</td>
                    </tr>
                </table>

                <h2>Memoization vs. Tabulation</h2>
                <ul>
                    <li><strong>Memoization (Top-Down)</strong>: Caches results of subproblems recursively
                        (e.g.,
                        Fibonacci).</li>
                    <li><strong>Tabulation (Bottom-Up)</strong>: Builds solutions iteratively using a table
                        (e.g.,
                        iterative DP).</li>
                </ul>

                <h2>Implementation Across Languages</h2>
                <ul>
                    <li><strong>Python</strong>: Use <code>@lru_cache</code> decorator from
                        <code>functools</code>.
                    </li>
                    <li><strong>JavaScript</strong>: Implement via closures or use libraries like Lodash.</li>
                    <li><strong>Java</strong>: Use <code>ConcurrentHashMap</code> for thread-safe caching.</li>
                </ul>
            </div>
        </div>
    </div>

</body>

</html>